{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#dqn.py \n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F # Often used for activation functions\n\nclass DQNetwork(nn.Module):\n    \"\"\"\n    Deep Q-Network for Reinforcement Learning.\n\n    This model substitutes the Q-table in Q-Learning. It takes state\n    information related to a number of devices and outputs Q-values for\n    all possible discrete actions.\n    \"\"\"\n    def __init__(self, num_devices: int, hidden_layer_list: list[int] = None,\n                 _input_features_override: int = None, # For RiskAverseDQN\n                 _output_features_override: int = None # For RiskAverseDQN\n                 ):\n        super().__init__()\n        self.num_devices = num_devices\n\n        if hidden_layer_list is None:\n            self.hidden_layer_list = [20, 20]\n        else:\n            self.hidden_layer_list = list(hidden_layer_list)\n\n        if _input_features_override is not None:\n            self.input_features = _input_features_override\n        else:\n            self.input_features = 4 * self.num_devices # Default\n\n        if _output_features_override is not None:\n            self.output_features = _output_features_override\n        else:\n            self.output_features = 3 ** self.num_devices # Default\n\n        layers = []\n        current_in_features = self.input_features\n        if self.hidden_layer_list:\n            for num_nodes in self.hidden_layer_list:\n                if num_nodes <= 0:\n                    raise ValueError(\"Number of nodes in a hidden layer must be positive.\")\n                layers.append(nn.Linear(current_in_features, num_nodes))\n                #layers.append(nn.BatchNorm1d(num_nodes))\n                layers.append(nn.LayerNorm(num_nodes))\n                layers.append(nn.ReLU())\n                current_in_features = num_nodes\n        \n        layers.append(nn.Linear(current_in_features, self.output_features))\n        self.network = nn.Sequential(*layers)\n\n    def forward(self, state: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the network.\n\n        Args:\n            state (torch.Tensor): The input state tensor.\n                                  Expected shape: (batch_size, 4 * num_devices)\n\n        Returns:\n            torch.Tensor: The Q-values for each action.\n                          Shape: (batch_size, 3^num_devices)\n        \"\"\"\n        if state.shape[-1] != self.input_features:\n            raise ValueError(\n                f\"Input tensor last dimension ({state.shape[-1]}) \"\n                f\"does not match expected input features ({self.input_features}).\"\n            )\n        return self.network(state)\n\n# --- Example Usage ---\nif __name__ == '__main__':\n    # Scenario 1: Default hidden layers\n    num_dev = 2\n    model1 = DQNetwork(num_devices=num_dev)\n    print(f\"Model 1 (num_devices={num_dev}, hidden_layers=default):\")\n    print(model1)\n    print(f\"  Input features: {model1.input_features} (4 * {num_dev})\")\n    print(f\"  Output features: {model1.output_features} (3^{num_dev})\")\n\n    # Create a dummy batch of input states\n    # batch_size = 5, num_devices = 2 => input_features = 4*2 = 8\n    dummy_input1 = torch.randn(5, 4 * num_dev)\n    output1 = model1(dummy_input1)\n    print(f\"  Dummy input shape: {dummy_input1.shape}\")\n    print(f\"  Output shape: {output1.shape}\")\n    print(\"-\" * 30)\n\n    # Scenario 2: Custom hidden layers\n    num_dev = 3\n    custom_hidden = [64, 32, 16]\n    model2 = DQNetwork(num_devices=num_dev, hidden_layer_list=custom_hidden)\n    print(f\"Model 2 (num_devices={num_dev}, hidden_layers={custom_hidden}):\")\n    print(model2)\n    print(f\"  Input features: {model2.input_features} (4 * {num_dev})\")\n    print(f\"  Output features: {model2.output_features} (3^{num_dev})\")\n\n    dummy_input2 = torch.randn(10, 4 * num_dev) # batch_size = 10\n    output2 = model2(dummy_input2)\n    print(f\"  Dummy input shape: {dummy_input2.shape}\")\n    print(f\"  Output shape: {output2.shape}\")\n    print(\"-\" * 30)\n\n    # Scenario 3: No hidden layers\n    num_dev = 1\n    model3 = DQNetwork(num_devices=num_dev, hidden_layer_list=[]) # Empty list\n    print(f\"Model 3 (num_devices={num_dev}, hidden_layers=[]):\")\n    print(model3)\n    print(f\"  Input features: {model3.input_features} (4 * {num_dev})\")\n    print(f\"  Output features: {model3.output_features} (3^{num_dev})\")\n\n    dummy_input3 = torch.randn(2, 4 * num_dev) # batch_size = 2\n    output3 = model3(dummy_input3)\n    print(f\"  Dummy input shape: {dummy_input3.shape}\")\n    print(f\"  Output shape: {output3.shape}\")\n    print(\"-\" * 30)\n\n    # Example of incorrect input shape\n    try:\n        num_dev_test = 2\n        model_test = DQNetwork(num_devices=num_dev_test)\n        wrong_input = torch.randn(1, 4 * num_dev_test + 1) # One extra feature\n        model_test(wrong_input)\n    except ValueError as e:\n        print(f\"Caught expected error for wrong input: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:51:26.284414Z","iopub.execute_input":"2025-06-20T06:51:26.285234Z","iopub.status.idle":"2025-06-20T06:51:26.304089Z","shell.execute_reply.started":"2025-06-20T06:51:26.285207Z","shell.execute_reply":"2025-06-20T06:51:26.303477Z"}},"outputs":[{"name":"stdout","text":"Model 1 (num_devices=2, hidden_layers=default):\nDQNetwork(\n  (network): Sequential(\n    (0): Linear(in_features=8, out_features=20, bias=True)\n    (1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n    (2): ReLU()\n    (3): Linear(in_features=20, out_features=20, bias=True)\n    (4): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n    (5): ReLU()\n    (6): Linear(in_features=20, out_features=9, bias=True)\n  )\n)\n  Input features: 8 (4 * 2)\n  Output features: 9 (3^2)\n  Dummy input shape: torch.Size([5, 8])\n  Output shape: torch.Size([5, 9])\n------------------------------\nModel 2 (num_devices=3, hidden_layers=[64, 32, 16]):\nDQNetwork(\n  (network): Sequential(\n    (0): Linear(in_features=12, out_features=64, bias=True)\n    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n    (2): ReLU()\n    (3): Linear(in_features=64, out_features=32, bias=True)\n    (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n    (5): ReLU()\n    (6): Linear(in_features=32, out_features=16, bias=True)\n    (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n    (8): ReLU()\n    (9): Linear(in_features=16, out_features=27, bias=True)\n  )\n)\n  Input features: 12 (4 * 3)\n  Output features: 27 (3^3)\n  Dummy input shape: torch.Size([10, 12])\n  Output shape: torch.Size([10, 27])\n------------------------------\nModel 3 (num_devices=1, hidden_layers=[]):\nDQNetwork(\n  (network): Sequential(\n    (0): Linear(in_features=4, out_features=3, bias=True)\n  )\n)\n  Input features: 4 (4 * 1)\n  Output features: 3 (3^1)\n  Dummy input shape: torch.Size([2, 4])\n  Output shape: torch.Size([2, 3])\n------------------------------\nCaught expected error for wrong input: Input tensor last dimension (9) does not match expected input features (8).\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"#riskaverseqlearning.py\n\nimport random\nimport numpy as np\nfrom collections import defaultdict\nimport itertools\nimport random\nimport math\n\nclass RiskAverseQLearning:\n    def __init__(self, K, N, M, I, Ts, d, St=1000):\n        self.num_devices = K\n        self.num_sub6 = N\n        self.num_mmWave = M\n        self.num_Qtable = I\n        self.frame_duration = Ts\n        self.packet_size = d\n        \n        self.cold_start = St\n        \n        self.exploration_rate = 0.999 # eps\n        self.decay_factor = 0.995 # lambda\n        self.discount_factor = 0.9 # gamma\n        \n        self.risk_control = 0.5 # lambda_p\n        self.utility_func_param = -0.5 # beta\n        \n        self.PLR_req = 0.1 # phi_max\n        self.Lk = [6] * K\n        \n        self.known_average_rate = [[self.packet_size / min(1,Ts) * self.Lk[_],self.packet_size / min(1,Ts) * self.Lk[_]] for _ in range(K)]\n        self.Success = [[0, 0] for _ in range(K)]\n        self.Alloc = [[0, 0] for _ in range(K)]\n        self.PLR = [[0.0, 0.0] for _ in range(K)]\n        self.PSR = [1.0 for _ in range(K)]\n        self.Q_table = [defaultdict(lambda:defaultdict(lambda:0.0)) for _ in range(I)]\n        self.Count = [defaultdict(lambda:defaultdict(int)) for _ in range(I)]\n        self.cur_state = None\n        self.init_state()\n    \n    \n        self.CC = defaultdict(int)\n    \n    def init_state(self):\n        \"\"\"\n        Initialize to a random state\n        \"\"\"\n        state = []\n        for k in range(self.num_devices):\n            a = random.choice(range(self.Lk[k]+1))\n            b = random.choice(range(self.Lk[k]+1))\n            state.extend([\n                random.choice([0,1]),\n                random.choice([0,1]),\n                a,\n                self.Lk[k]-a\n            ])\n        self.cur_state = tuple(state)\n    \n    def update_state(self):\n        \"\"\"\n        Update the current state based on PLR and Success\n        \"\"\"\n        state = []\n        for k in range(self.num_devices):\n            state.extend([\n                int(self.PLR[k][0] <= self.PLR_req),\n                int(self.PLR[k][1] <= self.PLR_req),\n                self.Success[k][0],\n                self.Success[k][1]\n                ])\n        self.cur_state = tuple(state)\n    \n    def get_random_action_tuple(self):\n        \"\"\"\n        Get a random {x}^k tuple, x in (0,1,2)\n        \"\"\"\n        return tuple(random.choices(range(3), k=self.num_devices))\n    \n    def get_action_tuple(self, Q_hat_index):\n        \"\"\"\n        Get an action when choose Q^H = Q_hat_index\n        \"\"\"\n        cur_state = self.cur_state\n        \n        # compute Q_hat explicitly\n        Q_bar = defaultdict(lambda:defaultdict(lambda:0.0))\n        for i in range(self.num_Qtable):\n            if cur_state not in self.Q_table[i]:\n                continue\n            q = self.Q_table[i][cur_state]\n            for a in q:\n                Q_bar[cur_state][a] += q[a]\n                \n        Q_hat = defaultdict(lambda:defaultdict(lambda:0.0))\n        #mx = 0\n        for a in itertools.product(range(3), repeat=self.num_devices):\n            # variance\n            Q = Q_bar[cur_state][a] / self.num_Qtable\n            for i in range(self.num_Qtable):\n                qval = 0\n                if cur_state in self.Q_table[i] and a in self.Q_table[i][cur_state]:\n                    qval = self.Q_table[i][cur_state][a]\n                Q_hat[cur_state][a] += (qval - Q)**2\n                            \n            # Q_hat\n            q = 0\n            if cur_state in self.Q_table[Q_hat_index] and a in self.Q_table[Q_hat_index][cur_state]:\n                q = self.Q_table[Q_hat_index][cur_state][a]\n            V = q - self.risk_control / max(1,(self.num_Qtable-1)) * Q_hat[cur_state][a]\n            Q_hat[cur_state][a] = V\n            #mx = max(mx, V)\n        a, v = self.get_max_action(Q_hat, cur_state)\n        #assert(mx >= v)\n        #if self.CC[cur_state] > 0:\n        #    print(cur_state, a, v, mx)\n        return a\n    \n    def receive_reward(self, reward, cur_frame, sample_achievable):\n        \"\"\"\n        Receive r(s,a), update to s'\n        \"\"\"\n        \n        # reward = array of [success sub6, success mmWave]\n        self.Success = reward\n        \n        total_reward = 0\n\n        # update PLR\n        for k in range(self.num_devices):\n            for i in range(2):\n                last_plr = self.PLR[k][i] * (cur_frame-1)\n                new_plr = 0\n                if self.Alloc[k][i] != 0:\n                    new_plr = 1 - self.Success[k][i] / self.Alloc[k][i]\n                self.PLR[k][i] = (last_plr + new_plr) / cur_frame\n        #print(\"PLR each device: \", [sum(x)/2 for x in self.PLR])\n        #print(\"PLR each device: \", self.PLR)\n        \n        # update PSR\n        for k in range(self.num_devices):\n            last_psr = self.PSR[k] * (cur_frame-1)\n            new_psr = 1\n            if sum(self.Alloc[k]) != 0:\n                new_psr = sum(self.Success[k]) / sum(self.Alloc[k])\n            self.PSR[k] = (last_psr + new_psr) /  cur_frame\n            \n            total_reward += self.PSR[k]\n            #total_reward += new_psr\n            \n            total_reward -= 1 - int(self.PLR[k][0] <= self.PLR_req)\n            total_reward -= 1 - int(self.PLR[k][1] <= self.PLR_req)\n        #print(\"PSR each device: \", self.PSR)\n        \n        # update known average rate\n        for k in range(self.num_devices):\n            for i in range(2):\n                alpha = 0.7\n                old_rate = self.known_average_rate[k][i] * alpha\n                new_rate = sample_achievable[k][i] * (1-alpha)\n                self.known_average_rate[k][i] = (old_rate + new_rate) \n        \n        return total_reward\n    \n    def map_action(self, action_chosen):\n        \"\"\"\n        Given tuple {x}^k, x in (0,1,2), map to [ <mm_i, sub6_i> ]\n        \"\"\"\n        for i, action in enumerate(action_chosen):\n            if action == 0:\n                self.Alloc[i][0] = max(1, min(int(self.known_average_rate[i][0] * self.frame_duration / self.packet_size), self.Lk[i]))\n                self.Alloc[i][1] = 0\n            elif action == 1:\n                self.Alloc[i][0] = 0\n                self.Alloc[i][1] = max(1, min(int(self.known_average_rate[i][1] * self.frame_duration / self.packet_size), self.Lk[i]))\n            else:\n                self.Alloc[i][1] = max(1, min(int(self.known_average_rate[i][1] * self.frame_duration / self.packet_size), self.Lk[i]))\n                self.Alloc[i][0] = max(1, min(int(self.known_average_rate[i][0] * self.frame_duration / self.packet_size), self.Lk[i] - self.Alloc[i][1]))\n        return self.Alloc\n    \n    def get_max_action(self, Q, s):\n        \"\"\"\n        Given Q and s, return [a, v] such that v = Q[s][a] max\n        \"\"\"\n        if s not in Q:\n            return [self.get_random_action_tuple(), 0]\n        state = Q[s]\n        curMaxNegAction = [None, -1e9]\n        curMaxPosAction = [None, -1e9]\n        negActionCount = 0\n        for action in itertools.product(range(3), repeat=self.num_devices):\n            if action not in state:\n                continue\n            v = state[action]\n            if v < 0:\n                if v > curMaxNegAction[1]:\n                    curMaxNegAction = [action, v]\n                negActionCount += 1\n            else:\n                if v > curMaxPosAction[1]:\n                    curMaxPosAction = [action, v]\n        if curMaxPosAction[1] > -1e9:\n            return curMaxPosAction\n        if negActionCount == 3**self.num_devices:\n            return curMaxNegAction\n        aList = []\n        for action in itertools.product(range(3), repeat=self.num_devices):\n            if action not in state:\n                aList.append(action)\n        return [random.choice(aList), 0]\n    \n    def get_current_action(self, cur_frame):\n        \"\"\"\n        Env ask for best action\n        \"\"\"\n        Q_hat_chosen = random.choice(range(self.num_Qtable))\n        if cur_frame >= self.cold_start:\n            if self.exploration_rate >= 0.01:\n                self.exploration_rate *= self.decay_factor\n        r1 = random.random()\n        if r1 < self.exploration_rate:\n            action_chosen = self.get_random_action_tuple()\n        else:\n            action_chosen = self.get_action_tuple(Q_hat_chosen)\n            \n        A = sum(k in (0, 2) for k in action_chosen)\n        B = sum(k in (1, 2) for k in action_chosen)\n            \n        return action_chosen, (A <= self.num_sub6 and B <= self.num_mmWave)\n        \n    def calc_utility(self, x):\n        return -math.exp(self.utility_func_param * x) \n        \n    def update_to_new_state(self, reward, cur_frame, action, sample_achievable_rate):\n        \"\"\"\n        Env send result of action a(t), calc r(t), update to s(t+1)\n        \"\"\"\n        old_state = self.cur_state\n        \n        rew = self.receive_reward(reward, cur_frame, sample_achievable_rate)\n        self.update_state()\n        \n        new_state = self.cur_state\n        #self.CC[new_state] += 1\n        #print(\"Old state: \", old_state)\n        #print(\"New state: \", new_state)\n        \n        # update table\n        msk = np.random.poisson(size=self.num_Qtable)\n        for i, v in enumerate(msk):\n            if v != 1:\n                continue\n            oldQ = 0\n            if old_state in self.Q_table[i] and action in self.Q_table[i][old_state]:\n                oldQ = self.Q_table[i][old_state][action]\n            oldA = 1\n            if old_state in self.Count[i] and action in self.Count[i][old_state]:\n                oldA = 1 / self.Count[i][old_state][action]\n            # find max Action in current Q (max[a] Q(s(t+1), *))\n            max_QA = self.get_max_action(self.Q_table[i], new_state)[1]\n            x0 = -10\n            newQ = oldQ + oldA * (self.calc_utility(rew + self.discount_factor * max_QA - oldQ) - x0) # eq (21)\n            \n            self.Count[i][old_state][action] += 1 # line 14+15            \n            self.Q_table[i][old_state][action] = newQ\n        return rew","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:51:26.489823Z","iopub.execute_input":"2025-06-20T06:51:26.490376Z","iopub.status.idle":"2025-06-20T06:51:26.518624Z","shell.execute_reply.started":"2025-06-20T06:51:26.490352Z","shell.execute_reply":"2025-06-20T06:51:26.518070Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# deepqlearing_simple.py\nimport random\nimport numpy as np\nfrom collections import deque, namedtuple\nimport itertools\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\n# Define a named tuple for transitions stored in the replay memory\nTransition = namedtuple('Transition', ('state', 'action_index', 'next_state', 'reward'))\n\nclass DeepQLearning:\n    def __init__(self, K, N, M, I, Ts, d,\n                 replay_memory_capacity=500,\n                 batch_size=64,\n                 gamma=0.9, # discount_factor\n                 eps_start=0.999, # exploration_rate start\n                 eps_end=0.05,\n                 eps_decay=0.995, # decay_factor for exploration_rate\n                 target_update_freq=100, # C from image (steps)\n                 learning_rate=1e-4,\n                 dqn_hidden_layers=None, # Pass to DQNetwork, e.g., [128, 64]\n                 St = 1500,\n                 ):\n        # Parameters from the original signature\n        self.num_devices = K\n        self.num_sub6 = N      # Max devices on sub6, used in get_current_action constraint check\n        self.num_mmWave = M    # Max devices on mmWave, used in get_current_action constraint check\n        # I (num_Qtable from RiskAverseQLearning) is unused but kept for signature compatibility\n        self.frame_duration = Ts\n        self.packet_size = d\n        self.cold_start = St\n        \n        # Q-learning and agent parameters\n        self.exploration_rate = eps_start # Current epsilon for epsilon-greedy\n        self.eps_start = eps_start\n        self.eps_end = eps_end\n        self.decay_factor = eps_decay # Multiplicative decay factor for exploration_rate\n        self.discount_factor = gamma  # GAMMA for Q-learning updates\n\n        # State and reward tracking, same as RiskAverseQLearning\n        self.PLR_req = 0.1\n        self.Lk = [6] * K # Example: Max packets per device\n        \n        self.known_average_rate = [[self.packet_size / min(1,Ts) * self.Lk[_],self.packet_size / min(1,Ts) * self.Lk[_]] for _ in range(K)] # [sub6, mmWave]\n        self.Success = [[0, 0] for _ in range(K)] # [sub6, mmWave] successful packets\n        self.Alloc = [[0, 0] for _ in range(K)]   # [sub6, mmWave] allocated packets\n        self.PLR = [[0.0, 0.0] for _ in range(K)] # [sub6, mmWave] Packet Loss Rate\n        self.PSR = [1.0 for _ in range(K)]        # Overall Packet Success Rate per device\n        \n        self.cur_state = None # This will be a Python tuple representing the current state\n        self.init_state()     # Initialize self.cur_state\n\n        # DQN specific attributes\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"Using device: {self.device}\")\n\n        # Initialize Policy Network and Target Network\n        self.policy_net = DQNetwork(num_devices=K, hidden_layer_list=dqn_hidden_layers).to(self.device)\n        self.target_net = DQNetwork(num_devices=K, hidden_layer_list=dqn_hidden_layers).to(self.device)\n        self.target_net.load_state_dict(self.policy_net.state_dict())\n        self.target_net.eval() # Target network is only for inference, not training directly\n\n        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=learning_rate, amsgrad=True)\n        self.replay_memory = deque(maxlen=replay_memory_capacity)\n        self.BATCH_SIZE = batch_size\n        self.TARGET_UPDATE_FREQUENCY = target_update_freq # C: steps to update target network\n        \n        self.total_steps_done = 0 # For decaying epsilon and updating target network\n\n        # Helper for mapping actions (tuples) to indices and vice-versa\n        # Action: tuple of length K, each element in {0, 1, 2}\n        # 0: sub6 only, 1: mmWave only, 2: both\n        self._action_tuples_list = list(itertools.product(range(3), repeat=self.num_devices))\n        self._action_to_index_map = {action_tuple: i for i, action_tuple in enumerate(self._action_tuples_list)}\n        self._index_to_action_map = {i: action_tuple for i, action_tuple in enumerate(self._action_tuples_list)}\n\n    def _state_to_tensor(self, state_tuple):\n        \"\"\"Converts a state tuple to a PyTorch tensor for network input.\"\"\"\n        if state_tuple is None:\n            return None\n        # Ensure the state is flat list of numbers before converting to tensor\n        return torch.tensor(list(state_tuple), dtype=torch.float32, device=self.device).unsqueeze(0)\n\n    def init_state(self):\n        \"\"\"Initialize to a random state (tuple). Kept from RiskAverseQLearning.\"\"\"\n        state_list = []\n        for k_idx in range(self.num_devices):\n            # Example state components:\n            # - PLR requirement met for sub6 (0 or 1)\n            # - PLR requirement met for mmWave (0 or 1)\n            # - Number of successful transmissions on sub6 in last step\n            # - Number of successful transmissions on mmWave in last step\n            # For init, we can use random values or typical starting values.\n            # The original used random choices for 'a' and 'b' for success counts.\n            # Let's use Lk[k_idx] as a reference for success counts init.\n            # Assuming Success components are counts, not rates for state.\n            sub6_success_init = random.choice(range(self.Lk[k_idx] + 1))\n            mmwave_success_init = self.Lk[k_idx] - sub6_success_init # Ensure sum is Lk for this example part\n            \n            state_list.extend([\n                random.choice([0, 1]),  # Mock PLR sub6 met\n                random.choice([0, 1]),  # Mock PLR mmWave met\n                sub6_success_init,      # Mock last success sub6 (count)\n                mmwave_success_init     # Mock last success mmWave (count)\n            ])\n        self.cur_state = tuple(state_list)\n\n    def update_state(self):\n        \"\"\"Update the current state (tuple) based on PLR and Success. Kept from RiskAverseQLearning.\"\"\"\n        state_list = []\n        for k_idx in range(self.num_devices):\n            state_list.extend([\n                int(self.PLR[k_idx][0] <= self.PLR_req),  # PLR sub6 requirement met\n                int(self.PLR[k_idx][1] <= self.PLR_req),  # PLR mmWave requirement met\n                self.Success[k_idx][0],                   # Actual successful packets sub6\n                self.Success[k_idx][1]                    # Actual successful packets mmWave\n            ])\n        self.cur_state = tuple(state_list)\n\n    def get_random_action_tuple(self):\n        \"\"\"Get a random action tuple. Kept from RiskAverseQLearning.\"\"\"\n        return tuple(random.choices(range(3), k=self.num_devices))\n\n    def get_action_tuple(self, state_tensor_for_net):\n        \"\"\"\n        Selects an action using the policy network based on the current state tensor.\n        Args:\n            state_tensor_for_net (torch.Tensor): The current state as a tensor [1, num_features].\n        Returns:\n            tuple: The action tuple selected by the policy network.\n        \"\"\"\n        with torch.no_grad(): # No gradient needed for action selection\n            # policy_net outputs Q-values for all 3^K actions\n            q_values = self.policy_net(state_tensor_for_net)\n            # Select action with the highest Q-value\n            action_index = q_values.max(1)[1].item() # .max(1) returns (values, indices)\n        return self._index_to_action_map[action_index]\n\n    def receive_reward(self, env_reward_signal, current_frame_number, sample_achievable_rates):\n        \"\"\"\n        Calculate scalar reward based on environment feedback and update internal metrics (PLR, PSR, known_average_rate).\n        Kept similar to RiskAverseQLearning.\n        Args:\n            env_reward_signal (list of lists): [[success_sub6_dev0, success_mmwave_dev0], ...]\n            current_frame_number (int): The current frame number (1-indexed).\n            sample_achievable_rates (list of lists): [[rate_sub6_dev0, rate_mmwave_dev0], ...]\n        Returns:\n            float: The calculated scalar reward.\n        \"\"\"\n        self.Success = env_reward_signal # Update success counts based on environment feedback\n        \n        total_scalar_reward = 0.0\n\n        # Update PLR for each device and each band\n        for k_idx in range(self.num_devices):\n            for band_idx in range(2): # 0 for sub6, 1 for mmWave\n                # Calculate sum of past PLR values to maintain running average\n                # current_frame_number is 1-indexed. For frame 1, (current_frame_number-1) is 0.\n                sum_past_plr = self.PLR[k_idx][band_idx] * (current_frame_number - 1)\n                \n                current_plr_value = 0.0\n                if self.Alloc[k_idx][band_idx] > 0:\n                    current_plr_value = 1.0 - (self.Success[k_idx][band_idx] / self.Alloc[k_idx][band_idx])\n                # If Alloc is 0, PLR is 0 (no packets sent, so no packets lost)\n                \n                self.PLR[k_idx][band_idx] = (sum_past_plr + current_plr_value) / current_frame_number\n        \n        # Update PSR (Packet Success Rate) and calculate part of the reward\n        for k_idx in range(self.num_devices):\n            sum_past_psr = self.PSR[k_idx] * (current_frame_number - 1)\n            \n            current_psr_value = 1.0 # Default PSR is 1 (e.g. if no packets allocated)\n            if sum(self.Alloc[k_idx]) > 0:\n                current_psr_value = sum(self.Success[k_idx]) / sum(self.Alloc[k_idx])\n            \n            self.PSR[k_idx] = (sum_past_psr + current_psr_value) / current_frame_number\n            \n            total_scalar_reward += current_psr_value\n            \n            # Penalize if PLR requirements are not met\n            total_scalar_reward -= (1 - int(self.PLR[k_idx][0] <= self.PLR_req)) # Penalty for sub6 PLR miss\n            total_scalar_reward -= (1 - int(self.PLR[k_idx][1] <= self.PLR_req)) # Penalty for mmWave PLR miss\n        \n        # Update known average achievable rate\n        for k_idx in range(self.num_devices):\n            for band_idx in range(2):\n                A = 0.7\n                sum_past_rates = self.known_average_rate[k_idx][band_idx] * A\n                current_rate_sample = sample_achievable_rates[k_idx][band_idx] * (1.0-A)\n                self.known_average_rate[k_idx][band_idx] = (sum_past_rates + current_rate_sample)\n        \n        return total_scalar_reward\n\n    def map_action(self, action_chosen_tuple):\n        \"\"\"\n        Maps the chosen action tuple to resource allocations (self.Alloc).\n        Kept from RiskAverseQLearning.\n        Args:\n            action_chosen_tuple (tuple): The action chosen, e.g., (0, 1, 2) for K devices.\n        Returns:\n            list of lists: The updated self.Alloc.\n        \"\"\"\n        for dev_idx, action_type in enumerate(action_chosen_tuple):\n            dev_lk = self.Lk[dev_idx] # Max packets for this device\n            # Estimated packets based on known average rate, frame duration, and packet size\n            est_packets_sub6 = int(self.known_average_rate[dev_idx][0] * self.frame_duration / self.packet_size)\n            est_packets_mmwave = int(self.known_average_rate[dev_idx][1] * self.frame_duration / self.packet_size)\n\n            if action_type == 0: # Sub6 only\n                self.Alloc[dev_idx][0] = max(0, min(est_packets_sub6, dev_lk))\n                self.Alloc[dev_idx][1] = 0\n            elif action_type == 1: # mmWave only\n                self.Alloc[dev_idx][0] = 0\n                self.Alloc[dev_idx][1] = max(0, min(est_packets_mmwave, dev_lk))\n            else: # action_type == 2, Both (prioritize mmWave up to Lk-1, then sub6)\n                # Allocate to mmWave, reserving at least 1 slot for sub6 if Lk > 0\n                alloc_mmwave_limit = dev_lk - 1 if dev_lk > 0 else 0\n                self.Alloc[dev_idx][1] = max(0, min(est_packets_mmwave, alloc_mmwave_limit))\n                \n                remaining_capacity_for_sub6 = dev_lk - self.Alloc[dev_idx][1]\n                self.Alloc[dev_idx][0] = max(0, min(est_packets_sub6, remaining_capacity_for_sub6))\n        return self.Alloc\n\n    def get_current_action(self, cur_frame):\n        \"\"\"\n        Selects an action using epsilon-greedy strategy:\n        With probability epsilon, selects a random action.\n        Otherwise, selects the best action according to the policy network.\n        Also returns a flag indicating if the action respects resource constraints.\n        \"\"\"\n        # Decay exploration rate\n        #self.exploration_rate = self.eps_end + \\\n        #                        (self.eps_start - self.eps_end) * \\\n        #                        math.exp(-1. * self.total_steps_done * (1./(1/self.decay_factor)) ) # Using typical exponential decay related to decay_factor interpreation\n\n        # Alternative simpler multiplicative decay:\n        if cur_frame >= self.cold_start:\n            if self.exploration_rate > self.eps_end:\n                self.exploration_rate *= self.decay_factor\n\n\n        rand_sample = random.random()\n        if rand_sample < self.exploration_rate:\n            action_chosen_tuple = self.get_random_action_tuple()\n            # print(f\"Step {self.total_steps_done}: RANDOM action! Eps: {self.exploration_rate:.3f}\") # For debugging\n        else:\n            current_state_tensor = self._state_to_tensor(self.cur_state)\n            action_chosen_tuple = self.get_action_tuple(current_state_tensor)\n            # print(f\"Step {self.total_steps_done}: DQN action. Eps: {self.exploration_rate:.3f}\") # For debugging\n            \n        # Check constraints (A: num devices on sub6, B: num devices on mmWave)\n        # Action type 0 (sub6), 1 (mmWave), 2 (both)\n        num_active_sub6 = sum(1 for k_act_type in action_chosen_tuple if k_act_type in (0, 2))\n        num_active_mmwave = sum(1 for k_act_type in action_chosen_tuple if k_act_type in (1, 2))\n            \n        constraints_met = (num_active_sub6 <= self.num_sub6 and num_active_mmwave <= self.num_mmWave)\n        \n        return action_chosen_tuple, constraints_met\n\n    def _optimize_model(self):\n        \"\"\"Performs a single step of optimization on the policy network using a batch from replay memory.\"\"\"\n        if len(self.replay_memory) < self.BATCH_SIZE:\n            return None # Not enough samples in memory to form a batch\n\n        # Sample a random minibatch of transitions from the replay memory\n        transitions = random.sample(self.replay_memory, self.BATCH_SIZE)\n        # Convert batch-array of Transitions to Transition of batch-arrays.\n        batch = Transition(*zip(*transitions))\n\n        # Concatenate batch elements for PyTorch processing\n        # batch.state, batch.next_state are tuples of tensors ([1, num_features])\n        state_batch = torch.cat(batch.state)\n        # batch.action_index is a tuple of integer action indices\n        action_batch = torch.tensor(batch.action_index, device=self.device, dtype=torch.long).unsqueeze(1)\n        # batch.reward is a tuple of scalar reward tensors ([1])\n        reward_batch = torch.cat(batch.reward)\n        next_state_batch = torch.cat(batch.next_state)\n        \n        # Compute Q(s_t, a_t)\n        current_q_values = self.policy_net(state_batch).gather(1, action_batch)\n\n        # Compute V(s_{t+1}) = max_{a'} Q_target(s_{t+1}, a')\n        with torch.no_grad():\n            next_state_max_q_values = self.target_net(next_state_batch).max(1)[0]\n        \n        # Compute the expected Q-values (y_j = r_j + gamma * V(s_{t+1}))\n        expected_q_values = reward_batch + (self.discount_factor * next_state_max_q_values)\n\n        loss = F.smooth_l1_loss(current_q_values, expected_q_values.unsqueeze(1))\n        # loss = F.mse_loss(current_q_values, expected_q_values.unsqueeze(1))\n\n        # Optimize the policy network\n        self.optimizer.zero_grad()\n        loss.backward()\n        # Optional: Gradient clipping to stabilize training\n        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n        self.optimizer.step()\n        \n        return loss.item() # Return loss value for monitoring\n\n    def update_to_new_state(self, env_reward_signal, current_frame_number, action_taken_tuple, sample_achievable_rates):\n        \"\"\"\n        Processes the outcome of an action:\n        1. Calculates the scalar reward and updates internal state metrics.\n        2. Updates the agent's current state (s_t -> s_{t+1}).\n        3. Stores the transition (s_t, a_t, r_t, s_{t+1}) in replay memory.\n        4. Performs a model optimization step (training).\n        5. Periodically updates the target network.\n        Args:\n            env_reward_signal (list of lists): Feedback from env (e.g., success counts).\n            current_frame_number (int): Current frame/timestep number.\n            action_taken_tuple (tuple): The action that was executed.\n            sample_achievable_rates (list of lists): Observed achievable rates.\n        Returns:\n            float: The scalar reward received for the transition.\n        \"\"\"\n        old_state_tuple = self.cur_state # s_t (Python tuple)\n        old_state_tensor = self._state_to_tensor(old_state_tuple) # Convert to tensor\n\n        # Calculate scalar reward (r_t) and update internal metrics based on env_reward_signal\n        actual_scalar_reward = self.receive_reward(env_reward_signal, current_frame_number, sample_achievable_rates)\n        reward_tensor = torch.tensor([actual_scalar_reward], device=self.device, dtype=torch.float32)\n        \n        # Update current state to new_state (s_{t+1}) based on updated metrics\n        self.update_state()\n        new_state_tuple = self.cur_state # s_{t+1} (Python tuple)\n        new_state_tensor = self._state_to_tensor(new_state_tuple) # Convert to tensor\n        \n        # Convert action_taken_tuple to its index for storage\n        action_index = self._action_to_index_map[action_taken_tuple]\n        \n        # Store the transition in replay memory D\n        self.replay_memory.append(Transition(old_state_tensor, action_index, new_state_tensor, reward_tensor))\n\n        # Increment step counter\n        self.total_steps_done += 1\n\n        # Perform one step of the optimization (on the policy network)\n        loss_value = self._optimize_model() # This will only run if memory has enough samples\n\n        # Periodically update the target network with weights from the policy network (every C steps)\n        if self.total_steps_done % self.TARGET_UPDATE_FREQUENCY == 0:\n            self.target_net.load_state_dict(self.policy_net.state_dict())\n            # print(f\"--- Step {self.total_steps_done}: Target network updated. Exploration: {self.exploration_rate:.4f} ---\")\n            # if loss_value is not None:\n            #    print(f\"Training Loss: {loss_value:.4f}\")\n\n        return actual_scalar_reward","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:51:26.519819Z","iopub.execute_input":"2025-06-20T06:51:26.520091Z","iopub.status.idle":"2025-06-20T06:51:26.552416Z","shell.execute_reply.started":"2025-06-20T06:51:26.520069Z","shell.execute_reply":"2025-06-20T06:51:26.551757Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"#deepqlearning_riskaverse.py\n\nimport random\nimport numpy as np\nfrom collections import deque, namedtuple\nimport itertools\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\n# Define a named tuple for transitions stored in the replay memory\n# s_orig: original state tuple from environment\n# eta_idx: index of the discrete eta value for the current state (η_t)\n# action_orig_idx: index of the original environment action (a_t)\n# next_eta_idx: index of the discrete eta value chosen as part of the action (η_{t+1})\n# reward_raw: the raw reward r_t from the environment\n# next_s_orig: original next state tuple from environment (s_{t+1})\nRiskTransition = namedtuple('RiskTransition', (\n    's_orig_tuple', 'current_eta_idx',\n    'action_orig_idx', 'chosen_next_eta_idx',\n    'reward_raw', 'next_s_orig_tuple'\n))\n\nclass RiskAverseDeepQLearning:\n    def __init__(self, K, N, M, I_unused, Ts, d,\n                 # --- Standard DQN params ---\n                 replay_memory_capacity=5000,\n                 batch_size=64,\n                 gamma=0.9, # discount_factor\n                 eps_start=0.999,\n                 eps_end=0.05,\n                 eps_decay=0.995,\n                 target_update_freq=100, # C from image (steps)\n                 learning_rate=1e-4,\n                 dqn_hidden_layers=None,\n                 St = 1500,\n                 # --- Risk-Averse Specific Params ---\n                 lambda_risk=0.5,  # λ_t in the paper (balancing expectation and CVaR)\n                 alpha_cvar=0.05,   # α_t, confidence level for CVaR (e.g., 0.05 for 95% CVaR)\n                 num_eta_levels=20, # D, number of discrete levels for η\n                 eta_min_val=-20.0, # Lower bound for η discretization (e.g., min expected reward)\n                 eta_max_val=20.0   # Upper bound for η discretization (e.g., max expected reward)\n                 ):\n\n        # Parameters from the original signature\n        self.num_devices = K\n        self.num_sub6 = N\n        self.num_mmWave = M\n        self.frame_duration = Ts\n        self.packet_size = d\n        self.cold_start = St\n        \n        # Q-learning and agent parameters\n        self.exploration_rate = eps_start\n        self.eps_start = eps_start\n        self.eps_end = eps_end\n        self.decay_factor = eps_decay\n        self.discount_factor = gamma # GAMMA\n\n        # State and reward tracking (original parts)\n        self.PLR_req = 0.1\n        self.Lk = [6] * K\n        self.known_average_rate =  [[self.packet_size / min(1,Ts) * self.Lk[_],self.packet_size / min(1,Ts) * self.Lk[_]] for _ in range(K)]\n        self.Success = [[0, 0] for _ in range(K)]\n        self.Alloc = [[0, 0] for _ in range(K)]\n        self.PLR = [[0.0, 0.0] for _ in range(K)]\n        self.PSR = [1.0 for _ in range(K)]\n        \n        self.cur_state_orig_tuple = None # Stores only the original environment state part\n\n        # --- Risk-Averse Specific Attributes ---\n        self.lambda_risk = lambda_risk\n        self.alpha_cvar = alpha_cvar\n        if not (0 < self.alpha_cvar <= 1):\n            raise ValueError(\"alpha_cvar must be in (0, 1]\")\n        if not (0 <= self.lambda_risk <= 1):\n            raise ValueError(\"lambda_risk must be in [0, 1]\")\n\n        self.num_eta_levels = num_eta_levels\n        self.eta_min_val = eta_min_val\n        self.eta_max_val = eta_max_val\n        if self.num_eta_levels > 1:\n            self.eta_discrete_values = torch.linspace(self.eta_min_val, self.eta_max_val, self.num_eta_levels)\n        elif self.num_eta_levels == 1:\n             self.eta_discrete_values = torch.tensor([(self.eta_min_val + self.eta_max_val) / 2.0])\n        else:\n            raise ValueError(\"num_eta_levels must be at least 1.\")\n        \n        self.current_eta_idx = self.num_eta_levels // 2\n        \n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"Using device: {self.device} for RiskAverseDQN\")\n        self.eta_discrete_values = self.eta_discrete_values.to(self.device)\n\n\n        # DQN specific attributes\n        # The input to DQNetwork will be: original_state_features + 1 (for current_eta_value)\n        # The output of DQNetwork will be: num_original_actions * num_eta_levels\n        self.num_original_actions = 3 ** self.num_devices\n        \n        # DQNetwork input_features = 4 * K (original state) + 1 (current eta value)\n        # DQNetwork output_features = (3^K actions) * num_eta_levels (for choosing next_eta_idx)\n        policy_input_features = 4 * self.num_devices + 1 \n        policy_output_features = self.num_original_actions * self.num_eta_levels\n\n        self.policy_net = DQNetwork(num_devices=self.num_devices, # This argument might be misleading for DQNetwork\n                                    hidden_layer_list=dqn_hidden_layers,\n                                    # Override input/output for risk-averse case\n                                    _input_features_override=policy_input_features,\n                                    _output_features_override=policy_output_features\n                                    ).to(self.device)\n        self.target_net = DQNetwork(num_devices=self.num_devices,\n                                    hidden_layer_list=dqn_hidden_layers,\n                                    _input_features_override=policy_input_features,\n                                    _output_features_override=policy_output_features\n                                    ).to(self.device)\n\n        self.target_net.load_state_dict(self.policy_net.state_dict())\n        self.target_net.eval()\n\n        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=learning_rate, amsgrad=True)\n        self.replay_memory = deque(maxlen=replay_memory_capacity)\n        self.BATCH_SIZE = batch_size\n        self.TARGET_UPDATE_FREQUENCY = target_update_freq\n        self.total_steps_done = 0\n\n        self._original_action_tuples_list = list(itertools.product(range(3), repeat=self.num_devices))\n        self._original_action_to_index_map = {action_tuple: i for i, action_tuple in enumerate(self._original_action_tuples_list)}\n        self._original_index_to_action_map = {i: action_tuple for i, action_tuple in enumerate(self._original_action_tuples_list)}\n        \n        self.init_state() # Initializes self.cur_state_orig_tuple\n\n    def _get_current_eta_value(self):\n        return self.eta_discrete_values[self.current_eta_idx]\n\n    def _state_to_tensor(self, state_orig_tuple, eta_value_tensor):\n        \"\"\"Converts original state tuple and eta_value tensor to a combined PyTorch tensor.\"\"\"\n        if state_orig_tuple is None: return None\n        s_tensor = torch.tensor(list(state_orig_tuple), dtype=torch.float32, device=self.device)\n        # Ensure eta_value_tensor is [1] or compatible for cat\n        if eta_value_tensor.ndim == 0:\n            eta_value_tensor = eta_value_tensor.unsqueeze(0)\n        return torch.cat((s_tensor, eta_value_tensor), dim=0).unsqueeze(0) # batch dim\n\n    # --- Functions to keep similar to DeepQLearning, but adapt for state/action ---\n\n    def init_state(self):\n        \"\"\"Initialize original state part. Eta is handled by self.current_eta_idx.\"\"\"\n        state_list = []\n        for k_idx in range(self.num_devices):\n            sub6_success_init = random.choice(range(self.Lk[k_idx] + 1))\n            mmwave_success_init = self.Lk[k_idx] - sub6_success_init\n            state_list.extend([\n                random.choice([0, 1]), random.choice([0, 1]),\n                sub6_success_init, mmwave_success_init\n            ])\n        self.cur_state_orig_tuple = tuple(state_list)\n        # Also reset current_eta_idx for a new episode/init\n        self.current_eta_idx = self.num_eta_levels // 2\n\n\n    def update_state(self):\n        \"\"\"Update original state part. Eta is updated based on action.\"\"\"\n        state_list = []\n        for k_idx in range(self.num_devices):\n            state_list.extend([\n                int(self.PLR[k_idx][0] <= self.PLR_req),\n                int(self.PLR[k_idx][1] <= self.PLR_req),\n                self.Success[k_idx][0], self.Success[k_idx][1]\n            ])\n        self.cur_state_orig_tuple = tuple(state_list)\n\n    def get_random_action_tuple(self):\n        \"\"\"Returns (random_original_action_tuple, random_next_eta_idx)\"\"\"\n        random_orig_action_tuple = tuple(random.choices(range(3), k=self.num_devices))\n        random_next_eta_idx = random.randrange(self.num_eta_levels)\n        return random_orig_action_tuple, random_next_eta_idx\n\n    def get_action_tuple(self, current_full_state_tensor):\n        \"\"\"\n        Selects (original_action_tuple, next_eta_idx) using the policy network.\n        Args:\n            current_full_state_tensor (torch.Tensor): Combined (original_state, current_eta_value).\n        Returns:\n            tuple: (chosen_original_action_tuple, chosen_next_eta_idx)\n        \"\"\"\n        with torch.no_grad():\n            q_values_all_composite_actions = self.policy_net(current_full_state_tensor) # Shape [1, num_orig_actions * num_eta_levels]\n            \n            # Find the index of the max Q-value (this is a flat index)\n            composite_action_flat_idx = q_values_all_composite_actions.argmax(dim=1).item()\n            \n            # Convert flat index back to (original_action_idx, next_eta_idx)\n            chosen_original_action_idx = composite_action_flat_idx // self.num_eta_levels\n            chosen_next_eta_idx = composite_action_flat_idx % self.num_eta_levels\n            \n            chosen_original_action_tuple = self._original_index_to_action_map[chosen_original_action_idx]\n        return chosen_original_action_tuple, chosen_next_eta_idx\n\n    # receive_reward and map_action remain identical to DeepQLearning as they deal with original rewards/actions\n    def receive_reward(self, env_reward_signal, current_frame_number, sample_achievable_rates):\n        self.Success = env_reward_signal\n        total_scalar_raw_reward = 0.0\n        for k_idx in range(self.num_devices):\n            for band_idx in range(2):\n                sum_past_plr = self.PLR[k_idx][band_idx] * (current_frame_number - 1)\n                current_plr_value = 0.0\n                if self.Alloc[k_idx][band_idx] > 0:\n                    current_plr_value = 1.0 - (self.Success[k_idx][band_idx] / self.Alloc[k_idx][band_idx])\n                self.PLR[k_idx][band_idx] = (sum_past_plr + current_plr_value) / current_frame_number\n        for k_idx in range(self.num_devices):\n            sum_past_psr = self.PSR[k_idx] * (current_frame_number - 1)\n            current_psr_value = 1.0\n            if sum(self.Alloc[k_idx]) > 0:\n                current_psr_value = sum(self.Success[k_idx]) / sum(self.Alloc[k_idx])\n            self.PSR[k_idx] = (sum_past_psr + current_psr_value) / current_frame_number\n            total_scalar_raw_reward += current_psr_value\n            total_scalar_raw_reward -= (1 - int(self.PLR[k_idx][0] <= self.PLR_req))\n            total_scalar_raw_reward -= (1 - int(self.PLR[k_idx][1] <= self.PLR_req))\n        for k_idx in range(self.num_devices):\n            for band_idx in range(2):\n                A = 0.7\n                sum_past_rates = self.known_average_rate[k_idx][band_idx] * A\n                current_rate_sample = sample_achievable_rates[k_idx][band_idx] * (1.0-A)\n                self.known_average_rate[k_idx][band_idx] = (sum_past_rates + current_rate_sample)\n        return total_scalar_raw_reward\n\n\n    def map_action(self, original_action_chosen_tuple):\n        # Uses the original action part to determine allocations\n        for dev_idx, action_type in enumerate(original_action_chosen_tuple):\n            dev_lk = self.Lk[dev_idx]\n            est_packets_sub6 = int(self.known_average_rate[dev_idx][0] * self.frame_duration / self.packet_size)\n            est_packets_mmwave = int(self.known_average_rate[dev_idx][1] * self.frame_duration / self.packet_size)\n            if action_type == 0:\n                self.Alloc[dev_idx][0] = max(0, min(est_packets_sub6, dev_lk))\n                self.Alloc[dev_idx][1] = 0\n            elif action_type == 1:\n                self.Alloc[dev_idx][0] = 0\n                self.Alloc[dev_idx][1] = max(0, min(est_packets_mmwave, dev_lk))\n            else:\n                alloc_mmwave_limit = dev_lk - 1 if dev_lk > 0 else 0\n                self.Alloc[dev_idx][1] = max(0, min(est_packets_mmwave, alloc_mmwave_limit))\n                remaining_capacity_for_sub6 = dev_lk - self.Alloc[dev_idx][1]\n                self.Alloc[dev_idx][0] = max(0, min(est_packets_sub6, remaining_capacity_for_sub6))\n        return self.Alloc\n\n    def get_current_action(self, cur_frame=0):\n        \"\"\"Selects (original_action_tuple, next_eta_idx) using epsilon-greedy.\"\"\"\n        #self.exploration_rate = self.eps_end + \\\n        #                        (self.eps_start - self.eps_end) * \\\n        #                        math.exp(-1. * self.total_steps_done * (1./(1/self.decay_factor)) )\n        if cur_frame >= self.cold_start:\n            if self.exploration_rate >= self.eps_end:\n                self.exploration_rate *= self.decay_factor\n        \n        rand_sample = random.random()\n        if rand_sample < self.exploration_rate:\n            chosen_orig_action_tuple, chosen_next_eta_idx = self.get_random_action_tuple()\n        else:\n            current_eta_val_tensor = self._get_current_eta_value().detach() # Ensure it's a scalar tensor\n            current_full_state_tensor = self._state_to_tensor(self.cur_state_orig_tuple, current_eta_val_tensor)\n            chosen_orig_action_tuple, chosen_next_eta_idx = self.get_action_tuple(current_full_state_tensor)\n            \n        # Constraint check based on original action part\n        num_active_sub6 = sum(1 for k_act_type in chosen_orig_action_tuple if k_act_type in (0, 2))\n        num_active_mmwave = sum(1 for k_act_type in chosen_orig_action_tuple if k_act_type in (1, 2))\n        constraints_met = (num_active_sub6 <= self.num_sub6 and num_active_mmwave <= self.num_mmWave)\n        \n        return chosen_orig_action_tuple, chosen_next_eta_idx, constraints_met\n\n    def _optimize_model(self):\n        if len(self.replay_memory) < self.BATCH_SIZE:\n            return None\n\n        transitions = random.sample(self.replay_memory, self.BATCH_SIZE)\n        batch = RiskTransition(*zip(*transitions))\n\n        # Unpack batch\n        s_orig_batch_tensor = torch.tensor(batch.s_orig_tuple, dtype=torch.float32, device=self.device)\n        current_eta_idx_batch = torch.tensor(batch.current_eta_idx, device=self.device, dtype=torch.long)\n        action_orig_idx_batch = torch.tensor(batch.action_orig_idx, device=self.device, dtype=torch.long)\n        chosen_next_eta_idx_batch = torch.tensor(batch.chosen_next_eta_idx, device=self.device, dtype=torch.long)\n        reward_raw_batch = torch.tensor(batch.reward_raw, device=self.device, dtype=torch.float32)\n        next_s_orig_batch_tensor = torch.tensor(batch.next_s_orig_tuple, dtype=torch.float32, device=self.device)\n        \n        # Get current eta values (η_j) from the discrete values tensor\n        current_eta_val_batch = self.eta_discrete_values[current_eta_idx_batch] # Shape [BATCH_SIZE]\n        \n        # Get next eta values chosen as action (η_{j+1})\n        chosen_next_eta_val_batch = self.eta_discrete_values[chosen_next_eta_idx_batch] # Shape [BATCH_SIZE]\n\n        # Prepare policy_net input: Concatenate original state tensor and current eta value\n        # Ensure eta_values have an extra dimension for concatenation if needed\n        policy_net_input_batch = torch.cat(\n            (s_orig_batch_tensor, current_eta_val_batch.unsqueeze(1)), dim=1\n        ) # Shape [BATCH_SIZE, num_orig_features + 1]\n\n        # Q( (s_j, η_j), (a_j, η_{j+1}) )\n        # Q_values for all (orig_action, next_eta) pairs for each state in batch\n        all_q_for_current_state = self.policy_net(policy_net_input_batch) # Shape [BATCH_SIZE, num_orig_actions * num_eta_levels]\n        \n        # Construct composite action index for gathering\n        composite_action_indices = action_orig_idx_batch * self.num_eta_levels + chosen_next_eta_idx_batch\n        current_q_values = all_q_for_current_state.gather(1, composite_action_indices.unsqueeze(1)).squeeze(1) # Shape [BATCH_SIZE]\n\n        # 1. Risk-adjusted immediate reward part: -(λ/α)[η_j - r_j]_+ + (1-λ)r_j\n        term1_cvar_part = -(self.lambda_risk / self.alpha_cvar) * F.relu(current_eta_val_batch - reward_raw_batch)\n        term1_exp_part = (1 - self.lambda_risk) * reward_raw_batch\n        risk_adjusted_immediate_reward = term1_cvar_part + term1_exp_part # Shape [BATCH_SIZE]\n\n        # 2. Middle term: γ * λ * η_{j+1}\n        term2_gamma_lambda_eta_next = self.discount_factor * self.lambda_risk * chosen_next_eta_val_batch # Shape [BATCH_SIZE]\n        \n        # 3. Future part: γ * max_{a', η''} Q_target(s_{j+1}, η_{j+1}, a', η''; θ_target)\n        target_net_input_batch = torch.cat(\n            (next_s_orig_batch_tensor, chosen_next_eta_val_batch.unsqueeze(1)), dim=1\n        ) # Shape [BATCH_SIZE, num_orig_features + 1]\n        \n        with torch.no_grad():\n            q_target_all_composite_actions = self.target_net(target_net_input_batch) # Shape [BATCH_SIZE, num_orig_actions * num_eta_levels]\n            max_q_target_next = q_target_all_composite_actions.max(dim=1)[0] # Max over all (a', η'') pairs. Shape [BATCH_SIZE]\n        \n        term3_gamma_max_q_target = self.discount_factor * max_q_target_next # Shape [BATCH_SIZE]\n        \n        # Combine for y_j\n        expected_q_values = risk_adjusted_immediate_reward + term2_gamma_lambda_eta_next + term3_gamma_max_q_target\n\n        # Compute loss\n        loss = F.smooth_l1_loss(current_q_values, expected_q_values)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n        self.optimizer.step()\n        \n        return loss.item()\n\n    def update_to_new_state(self, env_reward_signal, current_frame_number,\n                            action_taken_orig_tuple, chosen_next_eta_idx, # Action now has two parts\n                            sample_achievable_rates):\n        \n        old_original_state_tuple = self.cur_state_orig_tuple\n        previous_current_eta_idx = self.current_eta_idx # This is η_t\n\n        # Calculate raw reward and update original state metrics (PLR, PSR etc.)\n        raw_scalar_reward = self.receive_reward(env_reward_signal, current_frame_number, sample_achievable_rates)\n        \n        # Update original part of the state (s_t -> s_{t+1})\n        self.update_state()\n        new_original_state_tuple = self.cur_state_orig_tuple\n        \n        # Store transition in replay memory\n        # (s_orig_t, η_t_idx, a_orig_t_idx, η_{t+1}_idx, r_raw_t, s_orig_{t+1})\n        orig_action_idx = self._original_action_to_index_map[action_taken_orig_tuple]\n        \n        self.replay_memory.append(RiskTransition(\n            old_original_state_tuple, previous_current_eta_idx,\n            orig_action_idx, chosen_next_eta_idx,\n            raw_scalar_reward, new_original_state_tuple\n        ))\n\n        self.current_eta_idx = chosen_next_eta_idx\n        \n        self.total_steps_done += 1\n        loss_value = self._optimize_model()\n\n        if self.total_steps_done % self.TARGET_UPDATE_FREQUENCY == 0:\n            self.target_net.load_state_dict(self.policy_net.state_dict())\n            # if loss_value is not None:\n            #    print(f\"Step {self.total_steps_done}: Target net updated. RiskDQN Loss: {loss_value:.4f}, Exp: {self.exploration_rate:.3f}\")\n\n\n        return raw_scalar_reward # Return raw reward for external monitoring","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:51:26.695517Z","iopub.execute_input":"2025-06-20T06:51:26.695751Z","iopub.status.idle":"2025-06-20T06:51:26.728930Z","shell.execute_reply.started":"2025-06-20T06:51:26.695733Z","shell.execute_reply":"2025-06-20T06:51:26.728285Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import numpy as np\nimport json\nimport random\nimport os\nfrom datetime import datetime\nimport torch #sanity check\n\n# Matplotlib for plotting\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output # For live updates in notebooks\n\n# ---- Constants ----\nAP_COORD = (0, 0) # Access Point coordinates\nPOINT_SIZE = 5 # Used for grid alignment in original GUI, not directly for simulation\nPACKET_SIZE = 8000 # bits\nFRAME_DURATION = 1e-3 # seconds (1ms)\nUSE_RISK_AVERSE = False # Set to True to use RiskAverseQLearning\nUSE_DEEP_RISK = False # Set to True to use RiskAverseDeepQLearning, False for DeepQLearning (if USE_RISK_AVERSE is False)\nSCALE = 1.0 # Scale position (1.0: 1 block = 5m)\n\n# Suffix for output filenames based on selected Q-learning model\nSUFFIX_STR = \"\"\nif USE_RISK_AVERSE:\n    SUFFIX_STR = \"RA\"\nelse:\n    if USE_DEEP_RISK:\n        SUFFIX_STR = \"DEEPRA\"\n    else:\n        SUFFIX_STR = \"DEEP\"\n\n# ---- Network Simulation Core ----\n\nclass NetworkSimulator:\n    \"\"\"\n    Simulates a 5G network with an AP, devices, and blockages,\n    using a Q-learning agent for resource allocation.\n    \"\"\"\n    def __init__(self, json_config_path=None):\n        \"\"\"\n        Initializes the simulator.\n        \n        Args:\n            json_config_path (str, optional): Path to a JSON file containing\n                                               layout and simulation settings.\n        \"\"\"\n        # Data structures to hold device and blockage information\n        self.devices_data = [] \n        self.blockages_data = [] \n        \n        # Global simulation configuration parameters\n        self.global_config = self._default_config()\n\n        # Load configuration from JSON if provided\n        if json_config_path:\n            self.load_config_from_json(json_config_path)\n\n        # Simulation state variables\n        self.cur_frame = 0\n        self.distances = [] \n        self.nblocks = [] \n        self.plr = [] \n        self.RewList = [] \n        self.MetricList = [] \n        self.runnRew = 0 \n        self.qlearn = None \n        self.choice_counts = [] \n\n        # Data lists for Matplotlib plotting (stores (frame, value) tuples)\n        self.plot_data_metric = []\n        self.plot_data_reward = []\n        self.plot_data_plr_per_device = [] \n        self.plot_data_sub6_rate_per_device = []\n        self.plot_data_mmwave_rate_per_device = []\n\n        # Frequency for live plot updates (every N frames)\n        self.live_plot_update_interval = 500 \n\n    def _default_config(self):\n        \"\"\"Returns a dictionary of default global simulation parameters.\"\"\"\n        return {\n            \"pwr\": 5,             # Tx power (dBm)\n            \"noise\": -169,        # Noise power (dBm/Hz)\n            \"bw_sub6\": 100,       # Bandwidth Sub-6 (MHz)\n            \"bw_mm\": 1000,        # Bandwidth mmWave (MHz)\n            \"num_subchannel\": 4,  # Number of Sub-6GHz subchannels\n            \"num_beam\": 4,        # Number of mmWave beams\n            \"nframes\": 10000,     # Number of frames to simulate\n        }\n\n    def load_config_from_json(self, path):\n        \"\"\"\n        Loads network layout (devices, blockages) and global simulation settings\n        from a specified JSON file.\n        \"\"\"\n        if not os.path.exists(path):\n            print(f\"Error: JSON config file not found at {path}\")\n            return False\n\n        with open(path, \"r\") as f:\n            data = json.load(f)\n\n        # Clear existing data before loading new\n        self.devices_data.clear()\n        self.blockages_data.clear()\n\n        # Restore devices data\n        for d in data.get(\"devices\", []):\n            self.devices_data.append({\n                'pos': tuple(d[\"pos\"]),\n                'sub6_packets': d.get('sub6_packets', 0), \n                'mmwave_packets': d.get('mmwave_packets', 0), \n                'history': {\"sub6_success\": [], \"mmwave_success\": []} # Initialize history\n            })\n        # Restore blockages data\n        for b in data.get(\"blockages\", []):\n            self.blockages_data.append({'pos': tuple(b[\"pos\"])})\n\n        # Restore global config settings\n        s = data.get(\"settings\", {})\n        for key, value in s.items():\n            if key in self.global_config: \n                self.global_config[key] = value\n        \n        print(f\"Loaded configuration from {path}\")\n        print(f\"  Number of Devices: {len(self.devices_data)}\")\n        print(f\"  Number of Blockages: {len(self.blockages_data)}\")\n        print(f\"  Simulation Frames: {self.global_config['nframes']}\")\n        return True\n\n    def save_json(self, path):\n        \"\"\"\n        Saves the current layout (devices, blockages) and simulation settings\n        to a JSON file.\n        \"\"\"\n        data = {\n            \"devices\": [\n                {\"pos\": list(d['pos']), \"sub6_packets\": d['sub6_packets'], \"mmwave_packets\": d['mmwave_packets']}\n                for d in self.devices_data\n            ],\n            \"blockages\": [\n                {\"pos\": list(b['pos'])}\n                for b in self.blockages_data\n            ],\n            \"settings\": self.global_config\n        }\n        try:\n            with open(path, \"w\") as f:\n                json.dump(data, f, indent=2)\n            print(f\"Saved current configuration to {path}\")\n            return True\n        except Exception as e:\n            print(f\"Error saving JSON to {path}: {e}\")\n            return False\n\n    def setup_simulation(self):\n        \"\"\"\n        Initializes simulation-specific parameters (distances, blockages count,\n        PLR tracking) and instantiates the Q-learning agent.\n        \"\"\"\n        print(\"\\nSetting up simulation environment...\")\n        self.cur_frame = 0\n        self.distances = [0] * len(self.devices_data)\n        self.nblocks = [0] * len(self.devices_data)\n        \n        # Calculate initial distances and blockage counts for each device\n        for i, device in enumerate(self.devices_data):\n            dev_pos = [x * SCALE for x in device['pos']] #shrunk position for quick check\n            \n            # Distance from AP to device\n            dx = dev_pos[0] - AP_COORD[0]\n            dy = dev_pos[1] - AP_COORD[1]\n            sq_dist = dx**2 + dy**2\n            self.distances[i] = (sq_dist ** 0.50) if sq_dist > 0 else 1e-6 \n\n            # Count blockages between AP and device\n            for blockage in self.blockages_data:\n                blk_pos = [x * SCALE for x in blockage['pos']]\n                \n                # Check for collinearity\n                cross_product = (blk_pos[1] - AP_COORD[1]) * (dev_pos[0] - blk_pos[0]) - \\\n                                (blk_pos[0] - AP_COORD[0]) * (dev_pos[1] - blk_pos[1])\n                \n                if abs(cross_product) > 1e-6: \n                    continue\n                \n                # Check if blockage is between AP and device\n                v_ab_x = blk_pos[0] - AP_COORD[0]\n                v_ab_y = blk_pos[1] - AP_COORD[1]\n                v_ad_x = dev_pos[0] - AP_COORD[0]\n                v_ad_y = dev_pos[1] - AP_COORD[1]\n                \n                dot_product = v_ab_x * v_ad_x + v_ab_y * v_ad_y\n                len_v_ab_sq = v_ab_x**2 + v_ab_y**2\n                len_v_ad_sq = v_ad_x**2 + v_ad_y**2\n\n                if dot_product >= 0 and len_v_ab_sq <= len_v_ad_sq and len_v_ab_sq > 0:\n                    self.nblocks[i] += 1\n        \n        # Initialize lists for simulation tracking\n        self.plr = [0] * len(self.devices_data)\n        self.RewList = []\n        self.MetricList = []\n        self.runnRew = 0\n        \n        # Clear data collected for plotting from previous runs\n        self.plot_data_metric = []\n        self.plot_data_reward = []\n        self.plot_data_plr_per_device = [[] for _ in self.devices_data]\n        self.choice_counts = [[0, 0, 0] for _ in self.devices_data] # [sub6_count, mmWave_count, idle_count]\n        self.plot_data_sub6_rate_per_device = [[] for _ in self.devices_data]\n        self.plot_data_mmwave_rate_per_device = [[] for _ in self.devices_data]\n        \n        # Instantiate the Q-learning agent\n        num_devices = len(self.devices_data)\n        num_subchannels = self.global_config['num_subchannel']\n        num_beams = self.global_config['num_beam']\n\n        if USE_RISK_AVERSE:\n            self.qlearn = RiskAverseQLearning(num_devices, num_subchannels, num_beams, 4, FRAME_DURATION, PACKET_SIZE)\n        else:\n            if USE_DEEP_RISK:\n                self.qlearn = RiskAverseDeepQLearning(\n                    num_devices, num_subchannels, num_beams, 4,\n                    FRAME_DURATION, PACKET_SIZE,\n                    replay_memory_capacity = 2000,\n                    batch_size = 128,\n                    dqn_hidden_layers=[20, 20, 20], \n                    St = 1000,\n                    num_eta_levels=20,\n                    eta_min_val=-num_devices * 2,\n                    eta_max_val=num_devices \n                )\n            else:\n                self.qlearn = DeepQLearning(\n                    num_devices, num_subchannels, num_beams, 4,\n                    FRAME_DURATION, PACKET_SIZE,\n                    replay_memory_capacity = 2000,\n                    batch_size = 128,\n                    dqn_hidden_layers=[20, 20, 20],\n                    St = 1000\n                )\n\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            assert(self.qlearn.device == device)\n        print(\"Simulation setup complete.\")\n\n    def _db_to_linear(self, db):\n        \"\"\"Converts decibel (dB) value to linear scale.\"\"\"\n        return 10 ** (db / 10)\n\n    def _dbm_to_linear(self, dbm):\n        \"\"\"Converts dBm value to Watts (linear scale).\"\"\"\n        return self._db_to_linear(dbm - 30)\n\n    def _tx_beam_gain(self, theta, eps=0.1):\n        \"\"\"Calculates a simplified transmit beam gain.\"\"\"\n        theta = max(abs(theta), 1e-6) \n        return (2 * np.pi - (2 * np.pi - theta) * eps) / theta\n\n    def simulation_step(self):\n        \"\"\"\n        Executes one frame of the network simulation.\n        \"\"\"\n        num_devices = len(self.devices_data)\n        if num_devices == 0:\n            return \n\n        achievable_rate = [(0, 0)] * num_devices \n\n        P_tx_dbm = self.global_config['pwr']\n        P_tx_W = self._dbm_to_linear(P_tx_dbm)\n        \n        noise_density_dbm_hz = self.global_config['noise']\n        noise_density_W_hz = self._dbm_to_linear(noise_density_dbm_hz)\n\n        W_sub_MHz = self.global_config['bw_sub6']\n        W_sub_Hz = W_sub_MHz * 1e6\n        N_subchannels = max(1, self.global_config['num_subchannel'])\n        W_sub_per_channel_Hz = W_sub_Hz / N_subchannels\n        P_tx_sub_W = P_tx_W / N_subchannels \n        \n        W_mm_MHz = self.global_config['bw_mm']\n        W_mm_Hz = W_mm_MHz * 1e6\n        P_tx_mm_W = P_tx_W \n        \n        mmWave_tx_beamwidth_rad = 0.1\n        mmWave_tx_sidelobe_gain = 0.1 \n        mmWave_rx_gain_lin = 1 \n\n        for i, dev in enumerate(self.devices_data):\n            d = self.distances[i] \n\n            # --- Sub-6GHz Channel Model ---\n            PL_sub_db = 38.5 + 30 * np.log10(d) \n            PL_sub_lin = self._db_to_linear(-PL_sub_db)\n            h_small_scale_sub_power = np.random.rayleigh(scale=1.0)**2 \n            h_comb_sub_power = h_small_scale_sub_power * PL_sub_lin \n            noise_power_sub = noise_density_W_hz * W_sub_per_channel_Hz\n            gamma_sub = (P_tx_sub_W * h_comb_sub_power) / noise_power_sub\n            rate_sub_bps = W_sub_per_channel_Hz * np.log2(1 + gamma_sub)\n\n            # --- mmWave Channel Model ---\n            is_blocked = self.nblocks[i] > 0\n            if is_blocked: \n                shadowing_nlos_db = np.random.normal(0, 8.7) \n                PL_mm_db = 72 + 29.2 * np.log10(d) + shadowing_nlos_db\n            else: \n                shadowing_los_db = np.random.normal(0, 5.8) \n                PL_mm_db = 61.4 + 20 * np.log10(d) + shadowing_los_db\n            PL_mm_lin = self._db_to_linear(-PL_mm_db)\n            h_small_scale_mm_power = np.random.rayleigh(scale=1.0)**2\n            G_tx_lin = self._tx_beam_gain(mmWave_tx_beamwidth_rad, mmWave_tx_sidelobe_gain) \n            G_rx_lin = mmWave_rx_gain_lin \n            h_comb_mm_power = G_tx_lin * h_small_scale_mm_power * PL_mm_lin * G_rx_lin \n            noise_power_mm = noise_density_W_hz * W_mm_Hz\n            gamma_mm = (P_tx_mm_W * h_comb_mm_power) / noise_power_mm\n            rate_mm_bps = W_mm_Hz * np.log2(1 + gamma_mm)\n\n            achievable_rate[i] = (max(0, rate_sub_bps), max(0, rate_mm_bps))\n            \n        achievable_packets = [\n            list(map(lambda x: int(x * FRAME_DURATION / PACKET_SIZE), A))\n            for A in achievable_rate\n        ]\n        \n        q_learn_act = None\n        eta_idx = None\n        safe = True \n        \n        if not USE_RISK_AVERSE and USE_DEEP_RISK:\n            q_learn_act, eta_idx, safe = self.qlearn.get_current_action(self.cur_frame)\n        else:\n            q_learn_act, safe = self.qlearn.get_current_action(self.cur_frame)\n        \n        for d_idx, action_choice in enumerate(q_learn_act):\n            self.choice_counts[d_idx][action_choice] += 1\n        \n        action = self.qlearn.map_action(q_learn_act) \n\n        success = [[0, 0] for _ in range(num_devices)] \n        if safe: \n            success = [[min(act, achi) for act, achi in zip(a_proposed, a_achievable)]\n                       for a_proposed, a_achievable in zip(action, achievable_packets)]\n        \n        reward = 0\n        if not USE_RISK_AVERSE and USE_DEEP_RISK:\n            reward = self.qlearn.update_to_new_state(success, self.cur_frame, q_learn_act, eta_idx, achievable_rate)\n        else:\n            reward = self.qlearn.update_to_new_state(success, self.cur_frame, q_learn_act, achievable_rate)\n        \n        metric = 0 \n        for k in range(num_devices):\n            total_sent = sum(action[k])\n            total_received = sum(success[k])\n            \n            cur_psr = 0 \n            if total_sent > 0:\n                cur_psr = total_received / total_sent\n            cur_plr = 1 - cur_psr \n            \n            old_plr_sum = self.plr[k] * (self.cur_frame - 1) if self.cur_frame > 1 else 0\n            self.plr[k] = (old_plr_sum + cur_plr) / self.cur_frame if self.cur_frame > 0 else cur_plr\n            \n            metric += self.qlearn.PLR_req - self.plr[k] \n        metric /= num_devices \n        \n        self.RewList.append(reward) \n        self.MetricList.append(metric) \n        self.runnRew += reward \n\n        # Collect data for final and live plotting\n        self._collect_plot_data(metric, reward, self.plr, achievable_rate)\n\n        # Print periodic status updates AND update live plots\n        if self.cur_frame % self.live_plot_update_interval == 0 or self.cur_frame == 1:\n            clear_output(wait=True) # Clear previous output in the notebook cell\n            print(f\"--- Live Update: Frame {self.cur_frame}/{self.global_config['nframes']} ---\")\n            print(f\"  Current Reward: {reward:.4f} | Avg Reward (overall): {self.runnRew / max(1, self.cur_frame) / max(1, num_devices):.4f}\")\n            print(f\"  Current Metric (ΔP): {metric:.4f}\")\n            print(f\"  Avg PLR per device: {[f'{p:.4f}' for p in self.plr]}\")\n            \n            self._update_live_plots()\n\n    def _collect_plot_data(self, metric, reward, current_plr_list, current_achievable_rate_list):\n        \"\"\"\n        Internal method to collect simulation data points for later plotting.\n        \"\"\"\n        self.plot_data_metric.append((self.cur_frame, metric))\n        self.plot_data_reward.append((self.cur_frame, self.runnRew / max(1, self.cur_frame) / max(1, len(self.devices_data))))\n        \n        if not self.plot_data_plr_per_device: \n            self.plot_data_plr_per_device = [[] for _ in self.devices_data]\n        if not self.plot_data_sub6_rate_per_device or len(self.plot_data_sub6_rate_per_device) != len(self.devices_data):\n            self.plot_data_sub6_rate_per_device = [[] for _ in self.devices_data]\n        if not self.plot_data_mmwave_rate_per_device or len(self.plot_data_mmwave_rate_per_device) != len(self.devices_data):\n            self.plot_data_mmwave_rate_per_device = [[] for _ in self.devices_data]\n\n        for i, plr_val in enumerate(current_plr_list):\n            self.plot_data_plr_per_device[i].append((self.cur_frame, plr_val))\n        for i, (sub6_rate, mmwave_rate) in enumerate(current_achievable_rate_list):\n            self.plot_data_sub6_rate_per_device[i].append((self.cur_frame, sub6_rate / 1e6)) # Convert to Mbps for plotting\n            self.plot_data_mmwave_rate_per_device[i].append((self.cur_frame, mmwave_rate / 1e6)) # Convert to Mbps for plotting\n    \n    def _update_live_plots(self):\n        \"\"\"\n        Generates and displays live Matplotlib plots directly in the notebook cell.\n        Called periodically during the simulation.\n        \"\"\"\n        # Apply a nice Matplotlib style\n        plt.style.use('seaborn-v0_8-darkgrid')\n\n        # --- Plot 1: Metric (Delta P) ---\n        plt.figure(figsize=(10, 3))\n        if self.plot_data_metric:\n            frames, metrics = zip(*self.plot_data_metric)\n            plt.plot(frames, metrics, label=\"Metric (ΔP)\", color='blue')\n        plt.title(f\"Metric (ΔP) - Frame {self.cur_frame}\")\n        plt.xlabel(\"Frame\")\n        plt.ylabel(\"Metric (ΔP)\")\n        plt.legend()\n        plt.tight_layout()\n        plt.show()\n        plt.close() # Close figure to free memory\n\n        # --- Plot 2: Average Reward per Device ---\n        plt.figure(figsize=(10, 3))\n        if self.plot_data_reward:\n            frames, rewards = zip(*self.plot_data_reward)\n            plt.plot(frames, rewards, label=\"Average Reward per Device\", color='green')\n        plt.title(f\"Average Reward per Device - Frame {self.cur_frame}\")\n        plt.xlabel(\"Frame\")\n        plt.ylabel(\"Average Reward\")\n        plt.legend()\n        plt.tight_layout()\n        plt.show()\n        plt.close()\n\n        # --- Plot 3: Packet Loss Rate (PLR) per Device ---\n        plt.figure(figsize=(10, 3))\n        for i, device_plr_data in enumerate(self.plot_data_plr_per_device):\n            if device_plr_data:\n                frames, plrs = zip(*device_plr_data)\n                plt.plot(frames, plrs, label=f\"Device {i+1} PLR\")\n        \n        if self.qlearn and hasattr(self.qlearn, 'PLR_req'):\n            plt.axhline(y=self.qlearn.PLR_req, color='dimgray', linestyle='--', linewidth=1.2, label='PLR Max (Target)')\n        else: \n            plt.axhline(y=0.1, color='dimgray', linestyle='--', linewidth=1.2, label='Default PLR Max (0.1)')\n\n        plt.title(f\"Packet Loss Rate (PLR) - Frame {self.cur_frame}\")\n        plt.xlabel(\"Frame\")\n        plt.ylabel(\"PLR\")\n        plt.legend()\n        plt.tight_layout()\n        plt.show()\n        plt.close()\n\n        # --- Plot 4: Current Choice Distribution Bar Graph ---\n        plt.figure(figsize=(10, 3))\n        num_devices = len(self.devices_data)\n        \n        if num_devices > 0:\n            ind = np.arange(num_devices) \n            width = 0.2 \n            \n            final_choice_counts_array = np.array(self.choice_counts)\n            total_choices_per_device = final_choice_counts_array.sum(axis=1) \n            \n            normalized_counts = np.zeros_like(final_choice_counts_array, dtype=float)\n            for i in range(num_devices):\n                if total_choices_per_device[i] > 0:\n                    normalized_counts[i, :] = final_choice_counts_array[i, :] / total_choices_per_device[i]\n            \n            plt.bar(ind - width, normalized_counts[:, 0], width, label=\"Sub-6GHz\", color='skyblue')\n            plt.bar(ind, normalized_counts[:, 1], width, label=\"mmWave\", color='lightcoral')\n            plt.bar(ind + width, normalized_counts[:, 2], width, label=\"Idle\", color='lightgreen')\n\n            plt.title(f\"Choice Distribution - Frame {self.cur_frame}\")\n            plt.xlabel(\"Device\")\n            plt.ylabel(\"Ratio of Choices\")\n            plt.xticks(ind, [f\"Device {i+1}\" for i in range(num_devices)])\n            plt.legend()\n            plt.ylim(0, 1) \n            plt.tight_layout()\n            plt.show()\n            plt.close()\n\n        # --- Plot 5: Achievable Rate (Sub-6GHz) per Device ---\n        plt.figure(figsize=(10, 3))\n        for i, device_sub6_rate_data in enumerate(self.plot_data_sub6_rate_per_device):\n            if device_sub6_rate_data:\n                frames, rates = zip(*device_sub6_rate_data)\n                plt.plot(frames, rates, label=f\"Device {i+1}\")\n        \n        plt.title(f\"Achievable Rate (Sub-6GHz) - Frame {self.cur_frame}\")\n        plt.xlabel(\"Frame\")\n        plt.ylabel(\"Rate (Mbps)\")\n        plt.legend()\n        plt.tight_layout()\n        plt.show()\n        plt.close()\n\n        plt.figure(figsize=(10, 3))\n        for i, device_mmwave_rate_data in enumerate(self.plot_data_mmwave_rate_per_device):\n            if device_mmwave_rate_data:\n                frames, rates = zip(*device_mmwave_rate_data)\n                plt.plot(frames, rates, label=f\"Device {i+1}\")\n        \n        plt.title(f\"Achievable Rate (mmWave) - Frame {self.cur_frame}\")\n        plt.xlabel(\"Frame\")\n        plt.ylabel(\"Rate (Mbps)\")\n        plt.legend()\n        plt.tight_layout()\n        plt.show()\n        plt.close()\n\n    def run_simulation(self):\n        \"\"\"\n        Starts and runs the entire simulation process.\n        \"\"\"\n        if not self.devices_data:\n            print(\"No devices configured. Please load a JSON layout or manually add device data.\")\n            return\n\n        self.setup_simulation() \n        \n        nframes = self.global_config['nframes']\n        print(f\"\\nStarting simulation for {nframes} frames...\")\n\n        for frame in range(nframes):\n            self.cur_frame = frame + 1 \n            self.simulation_step() \n        \n        clear_output(wait=True) # Clear final live plot to show only saved plots and final log\n        print(\"\\nSimulation finished.\")\n        self.generate_and_save_outputs() \n\n    def generate_and_save_outputs(self):\n        \"\"\"\n        Generates Matplotlib plots of simulation metrics and saves them as images.\n        Also saves a comprehensive text log file. This is for the *final* plots.\n        \"\"\"\n        timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        output_dir = \"/kaggle/working/output/\"\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        \n        plt.style.use('seaborn-v0_8-darkgrid') \n\n        # --- Plot 1: Metric (Delta P) over Frames ---\n        plt.figure(figsize=(10, 6))\n        if self.plot_data_metric:\n            frames, metrics = zip(*self.plot_data_metric)\n            plt.plot(frames, metrics, label=\"Metric (ΔP)\", color='blue')\n        plt.title(\"Final Metric (ΔP) over Frames\")\n        plt.xlabel(\"Frame\")\n        plt.ylabel(\"Metric (ΔP)\")\n        plt.legend()\n        plt.tight_layout()\n        metric_filename = os.path.join(output_dir, f\"{timestamp_str}_metric_{SUFFIX_STR}.png\")\n        plt.savefig(metric_filename)\n        plt.close() \n        print(f\"Saved figure: {metric_filename}\")\n\n        # --- Plot 2: Average Reward per Device over Frames ---\n        plt.figure(figsize=(10, 6))\n        if self.plot_data_reward:\n            frames, rewards = zip(*self.plot_data_reward)\n            plt.plot(frames, rewards, label=\"Average Reward per Device\", color='green')\n        plt.title(\"Final Average Reward per Device over Frames\")\n        plt.xlabel(\"Frame\")\n        plt.ylabel(\"Average Reward\")\n        plt.legend()\n        plt.tight_layout()\n        reward_filename = os.path.join(output_dir, f\"{timestamp_str}_reward_{SUFFIX_STR}.png\")\n        plt.savefig(reward_filename)\n        plt.close()\n        print(f\"Saved figure: {reward_filename}\")\n\n        # --- Plot 3: Packet Loss Rate (PLR) per Device over Frames ---\n        plt.figure(figsize=(10, 6))\n        for i, device_plr_data in enumerate(self.plot_data_plr_per_device):\n            if device_plr_data:\n                frames, plrs = zip(*device_plr_data)\n                plt.plot(frames, plrs, label=f\"Device {i+1} PLR\")\n        \n        if self.qlearn and hasattr(self.qlearn, 'PLR_req'):\n            plt.axhline(y=self.qlearn.PLR_req, color='dimgray', linestyle='--', linewidth=1.2, label='PLR Max (Target)')\n        else: \n            plt.axhline(y=0.1, color='dimgray', linestyle='--', linewidth=1.2, label='Default PLR Max (0.1)')\n\n        plt.title(\"Final Packet Loss Rate (PLR) per Device\")\n        plt.xlabel(\"Frame\")\n        plt.ylabel(\"PLR\")\n        plt.legend()\n        plt.tight_layout()\n        plr_filename = os.path.join(output_dir, f\"{timestamp_str}_plr_{SUFFIX_STR}.png\")\n        plt.savefig(plr_filename)\n        plt.close()\n        print(f\"Saved figure: {plr_filename}\")\n\n        # --- Plot 4: Final Choice Distribution Bar Graph ---\n        plt.figure(figsize=(10, 6))\n        num_devices = len(self.devices_data)\n        \n        if num_devices > 0:\n            ind = np.arange(num_devices) \n            width = 0.2 \n            \n            final_choice_counts_array = np.array(self.choice_counts)\n            total_choices_per_device = final_choice_counts_array.sum(axis=1) \n            \n            normalized_counts = np.zeros_like(final_choice_counts_array, dtype=float)\n            for i in range(num_devices):\n                if total_choices_per_device[i] > 0:\n                    normalized_counts[i, :] = final_choice_counts_array[i, :] / total_choices_per_device[i]\n            \n            plt.bar(ind - width, normalized_counts[:, 0], width, label=\"Sub-6GHz\", color='skyblue')\n            plt.bar(ind, normalized_counts[:, 1], width, label=\"mmWave\", color='lightcoral')\n            plt.bar(ind + width, normalized_counts[:, 2], width, label=\"Idle\", color='lightgreen')\n\n            plt.title(\"Final Choice Distribution (Ratio of Total Choices)\")\n            plt.xlabel(\"Device\")\n            plt.ylabel(\"Ratio of Choices\")\n            plt.xticks(ind, [f\"Device {i+1}\" for i in range(num_devices)])\n            plt.legend()\n            plt.ylim(0, 1) \n            plt.tight_layout()\n            choice_filename = os.path.join(output_dir, f\"{timestamp_str}_choice_distribution_{SUFFIX_STR}.png\")\n            plt.savefig(choice_filename)\n            plt.close()\n            print(f\"Saved figure: {choice_filename}\")\n        else:\n            print(\"Skipping Choice Distribution plot: No devices found.\")\n\n        # --- Plot 5: Final Achievable Rate (Sub-6GHz) per Device ---\n        plt.figure(figsize=(10, 6))\n        for i, device_sub6_rate_data in enumerate(self.plot_data_sub6_rate_per_device):\n            if device_sub6_rate_data:\n                frames, rates = zip(*device_sub6_rate_data)\n                plt.plot(frames, rates, label=f\"Device {i+1} Sub-6GHz Rate\")\n        \n        plt.title(\"Final Achievable Rate (Sub-6GHz) per Device\")\n        plt.xlabel(\"Frame\")\n        plt.ylabel(\"Rate (Mbps)\")\n        plt.legend()\n        plt.tight_layout()\n        sub6_rate_filename = os.path.join(output_dir, f\"{timestamp_str}_sub6_rate_{SUFFIX_STR}.png\")\n        plt.savefig(sub6_rate_filename)\n        plt.close()\n        print(f\"Saved figure: {sub6_rate_filename}\")\n\n        # --- Plot 6: Final Achievable Rate (mmWave) per Device ---\n        plt.figure(figsize=(10, 6))\n        for i, device_mmwave_rate_data in enumerate(self.plot_data_mmwave_rate_per_device):\n            if device_mmwave_rate_data:\n                frames, rates = zip(*device_mmwave_rate_data)\n                plt.plot(frames, rates, label=f\"Device {i+1} mmWave Rate\")\n        \n        plt.title(\"Final Achievable Rate (mmWave) per Device\")\n        plt.xlabel(\"Frame\")\n        plt.ylabel(\"Rate (Mbps)\")\n        plt.legend()\n        plt.tight_layout()\n        mmwave_rate_filename = os.path.join(output_dir, f\"{timestamp_str}_mmwave_rate_{SUFFIX_STR}.png\")\n        plt.savefig(mmwave_rate_filename)\n        plt.close()\n        print(f\"Saved figure: {mmwave_rate_filename}\")\n\n        # --- Save Simulation Log File ---\n        log_filepath = os.path.join(output_dir, f'{timestamp_str}_log_{SUFFIX_STR}.txt')\n        with open(log_filepath, 'w+') as f:\n            f.write(f'--- Simulation Log ---\\n')\n            f.write(f'Timestamp: {timestamp_str}\\n')\n            f.write(f'Q-Learning Model: {\"RiskAverseQLearning\" if USE_RISK_AVERSE else \"RiskAverseDeepQLearning\" if USE_DEEP_RISK else \"DeepQLearning\"}\\n')\n            f.write(f'Number of Devices: {len(self.devices_data)}\\n')\n            f.write(f'Number of Blockages: {len(self.blockages_data)}\\n')\n            f.write(f'Simulated Frames: {self.global_config[\"nframes\"]}\\n')\n            f.write(f'\\n--- Global Configuration ---\\n')\n            for k, v in self.global_config.items():\n                f.write(f'{k}: {v}\\n')\n            f.write(f'\\n--- Device Details ---\\n')\n            for i, dev in enumerate(self.devices_data):\n                f.write(f'Device {i+1}: Position={dev[\"pos\"]}, Distance={self.distances[i]:.2f}m, Blockages={self.nblocks[i]}\\n')\n            f.write(f'\\n--- Final Simulation Results ---\\n')\n            \n            final_avg_plr = sum(self.plr) / len(self.devices_data) if len(self.devices_data) > 0 else 0\n            f.write(f'Final Average PLR across all devices: {final_avg_plr:.4f}\\n')\n            f.write(f'PLR for each device: [ {\" \".join(f\"{x:.4f}\" for x in self.plr)} ]\\n')\n            \n            avg_success = (len(self.devices_data) - sum(self.plr)) / len(self.devices_data) if len(self.devices_data) > 0 else 0\n            f.write(f'Average Success Rate across all devices: {avg_success:.4f}\\n')\n            \n            final_avg_reward_per_frame_per_device = self.plot_data_reward[-1][1] if self.plot_data_reward else 0\n            f.write(f'Final Average Reward per Device per Frame: {final_avg_reward_per_frame_per_device:.4f}\\n')\n            \n            final_metric = self.MetricList[-1] if self.MetricList else 0\n            f.write(f'Final Metric (ΔP): {final_metric:.4f}\\n')\n\n            f.write(f'\\n--- Achievable Rates (Mbps) ---\\n')\n            if len(self.devices_data) > 0:\n                avg_sub6_rates = [np.mean([rate for _, rate in dev_data]) for dev_data in self.plot_data_sub6_rate_per_device if dev_data]\n                avg_mmwave_rates = [np.mean([rate for _, rate in dev_data]) for dev_data in self.plot_data_mmwave_rate_per_device if dev_data]\n                \n                f.write(f'Average Sub-6GHz Rates per device: [ {\" \".join(f\"{x:.2f}\" for x in avg_sub6_rates)} ] Mbps\\n')\n                f.write(f'Average mmWave Rates per device: [ {\" \".join(f\"{x:.2f}\" for x in avg_mmwave_rates)} ] Mbps\\n')\n                \n                overall_avg_sub6_rate = np.mean(avg_sub6_rates) if avg_sub6_rates else 0\n                overall_avg_mmwave_rate = np.mean(avg_mmwave_rates) if avg_mmwave_rates else 0\n                f.write(f'Overall Average Sub-6GHz Rate: {overall_avg_sub6_rate:.2f} Mbps\\n')\n                f.write(f'Overall Average mmWave Rate: {overall_avg_mmwave_rate:.2f} Mbps\\n')\n            else:\n                f.write(f'No device data available for rate calculations.\\n')\n            \n            f.write(f'\\n--- Data per Frame (Metric | Reward) ---\\n')\n            for f_idx in range(len(self.MetricList)):\n                f.write(f'Frame {f_idx+1}: Metric={self.MetricList[f_idx]:.4f} | Reward={self.RewList[f_idx]:.4f}\\n')\n                if self.plot_data_sub6_rate_per_device and self.plot_data_mmwave_rate_per_device:\n                    f.write(f'  Rates (Mbps): ')\n                    for d_idx in range(len(self.devices_data)):\n                        sub6_r = self.plot_data_sub6_rate_per_device[d_idx][f_idx][1] if f_idx < len(self.plot_data_sub6_rate_per_device[d_idx]) else 0\n                        mmwave_r = self.plot_data_mmwave_rate_per_device[d_idx][f_idx][1] if f_idx < len(self.plot_data_mmwave_rate_per_device[d_idx]) else 0\n                        f.write(f'Device {d_idx+1}(S6:{sub6_r:.2f}, MM:{mmwave_r:.2f}) ')\n                    f.write('\\n')\n        print(f\"Saved log: {log_filepath}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:51:26.730638Z","iopub.execute_input":"2025-06-20T06:51:26.730858Z","iopub.status.idle":"2025-06-20T06:51:27.003109Z","shell.execute_reply.started":"2025-06-20T06:51:26.730842Z","shell.execute_reply":"2025-06-20T06:51:27.002450Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"#gpu check\nprint(torch.cuda.is_available())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:51:27.004034Z","iopub.execute_input":"2025-06-20T06:51:27.004273Z","iopub.status.idle":"2025-06-20T06:51:27.024745Z","shell.execute_reply.started":"2025-06-20T06:51:27.004253Z","shell.execute_reply":"2025-06-20T06:51:27.024051Z"}},"outputs":[{"name":"stdout","text":"True\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"#json\n\n#one_json\none_json=\"\"\"{\n  \"devices\": [\n    {\n      \"pos\": [\n        0,\n        -20\n      ]\n    },\n    {\n      \"pos\": [\n        20,\n        0\n      ]\n    },\n    {\n      \"pos\": [\n        -60,\n        60\n      ]\n    }\n  ],\n  \"blockages\": [\n    {\n      \"pos\": [\n        10,\n        0\n      ]\n    }\n  ],\n  \"settings\": {\n    \"pwr\": 5.0,\n    \"noise\": -169.0,\n    \"bw_sub6\": 100.0,\n    \"bw_mm\": 1000.0,\n    \"num_subchannel\": 4,\n    \"num_beam\": 4,\n    \"nframes\": 10000\n  }\n}\"\"\"\n\ntwo_json=\"\"\"{\n  \"devices\": [\n    {\n      \"pos\": [\n        0,\n        -20\n      ]\n    },\n    {\n      \"pos\": [\n        20,\n        0\n      ]\n    },\n    {\n      \"pos\": [\n        -60,\n        60\n      ]\n    },\n    {\n      \"pos\": [\n        -40,\n        -40\n      ]\n    },\n    {\n      \"pos\": [\n        15,\n        60\n      ]\n    },\n    {\n      \"pos\": [\n        -20,\n        20\n      ]\n    },\n    {\n      \"pos\": [\n        -40,\n        -10\n      ]\n    },\n    {\n      \"pos\": [\n        55,\n        -55\n      ]\n    },\n    {\n      \"pos\": [\n        50,\n        0\n      ]\n    },\n    {\n      \"pos\": [\n        50,\n        40\n      ]\n    }\n  ],\n  \"blockages\": [\n    {\n      \"pos\": [\n        10,\n        0\n      ]\n    },\n    {\n      \"pos\": [\n        -10,\n        10\n      ]\n    }\n  ],\n  \"settings\": {\n    \"pwr\": 5.0,\n    \"noise\": -169.0,\n    \"bw_sub6\": 100.0,\n    \"bw_mm\": 1000.0,\n    \"num_subchannel\": 16,\n    \"num_beam\": 16,\n    \"nframes\": 10000\n  }\n}\"\"\"\n\nif not os.path.exists(\"/kaggle/working/input/\"):\n    os.makedirs(\"/kaggle/working/input/\")\nwith open(\"/kaggle/working/input/1.json\", \"w\") as f:\n    f.write(one_json)\nwith open(\"/kaggle/working/input/2.json\", \"w\") as f:\n    f.write(two_json)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:51:27.025464Z","iopub.execute_input":"2025-06-20T06:51:27.025664Z","iopub.status.idle":"2025-06-20T06:51:27.039384Z","shell.execute_reply.started":"2025-06-20T06:51:27.025649Z","shell.execute_reply":"2025-06-20T06:51:27.038883Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"\n\njson_file_path = \"/kaggle/working/input/2.json\"\n\nfor _ in range(5):\n    simulator = NetworkSimulator(json_config_path=json_file_path)\n    simulator.run_simulation()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:51:27.040822Z","iopub.execute_input":"2025-06-20T06:51:27.041063Z","iopub.status.idle":"2025-06-20T06:59:22.518115Z","shell.execute_reply.started":"2025-06-20T06:51:27.041045Z","shell.execute_reply":"2025-06-20T06:59:22.517312Z"}},"outputs":[{"name":"stdout","text":"\nSimulation finished.\nSaved figure: /kaggle/working/output/20250620_065915_metric_DEEP.png\nSaved figure: /kaggle/working/output/20250620_065915_reward_DEEP.png\nSaved figure: /kaggle/working/output/20250620_065915_plr_DEEP.png\nSaved figure: /kaggle/working/output/20250620_065915_choice_distribution_DEEP.png\nSaved figure: /kaggle/working/output/20250620_065915_sub6_rate_DEEP.png\nSaved figure: /kaggle/working/output/20250620_065915_mmwave_rate_DEEP.png\nSaved log: /kaggle/working/output/20250620_065915_log_DEEP.txt\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# save output\n!zip -r /kaggle/working/output.zip /kaggle/working/output\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:59:22.519134Z","iopub.execute_input":"2025-06-20T06:59:22.519379Z","iopub.status.idle":"2025-06-20T06:59:25.151724Z","shell.execute_reply.started":"2025-06-20T06:59:22.519359Z","shell.execute_reply":"2025-06-20T06:59:25.150792Z"}},"outputs":[{"name":"stdout","text":"updating: kaggle/working/output/ (stored 0%)\nupdating: kaggle/working/output/20250620_055646_sub6_rate_DEEPRA.png (deflated 2%)\nupdating: kaggle/working/output/20250620_060650_log_DEEPRA.txt (deflated 77%)\nupdating: kaggle/working/output/20250620_061152_metric_DEEPRA.png (deflated 17%)\nupdating: kaggle/working/output/20250620_060650_mmwave_rate_DEEPRA.png (deflated 4%)\nupdating: kaggle/working/output/20250620_060153_plr_DEEPRA.png (deflated 9%)\nupdating: kaggle/working/output/20250620_061152_sub6_rate_DEEPRA.png (deflated 3%)\nupdating: kaggle/working/output/20250620_061152_choice_distribution_DEEPRA.png (deflated 22%)\nupdating: kaggle/working/output/20250620_060650_sub6_rate_DEEPRA.png (deflated 3%)\nupdating: kaggle/working/output/20250620_060153_choice_distribution_DEEPRA.png (deflated 22%)\nupdating: kaggle/working/output/20250620_060153_reward_DEEPRA.png (deflated 12%)\nupdating: kaggle/working/output/20250620_060153_mmwave_rate_DEEPRA.png (deflated 4%)\nupdating: kaggle/working/output/20250620_061152_log_DEEPRA.txt (deflated 77%)\nupdating: kaggle/working/output/20250620_060650_choice_distribution_DEEPRA.png (deflated 22%)\nupdating: kaggle/working/output/20250620_055646_metric_DEEPRA.png (deflated 17%)\nupdating: kaggle/working/output/20250620_060153_metric_DEEPRA.png (deflated 19%)\nupdating: kaggle/working/output/20250620_055646_reward_DEEPRA.png (deflated 11%)\nupdating: kaggle/working/output/20250620_055646_choice_distribution_DEEPRA.png (deflated 23%)\nupdating: kaggle/working/output/20250620_060650_plr_DEEPRA.png (deflated 9%)\nupdating: kaggle/working/output/20250620_061152_mmwave_rate_DEEPRA.png (deflated 4%)\nupdating: kaggle/working/output/20250620_060650_metric_DEEPRA.png (deflated 17%)\nupdating: kaggle/working/output/20250620_060650_reward_DEEPRA.png (deflated 17%)\nupdating: kaggle/working/output/20250620_060153_sub6_rate_DEEPRA.png (deflated 3%)\nupdating: kaggle/working/output/20250620_060153_log_DEEPRA.txt (deflated 77%)\nupdating: kaggle/working/output/20250620_055646_plr_DEEPRA.png (deflated 9%)\nupdating: kaggle/working/output/20250620_055646_log_DEEPRA.txt (deflated 77%)\nupdating: kaggle/working/output/20250620_061152_plr_DEEPRA.png (deflated 8%)\nupdating: kaggle/working/output/20250620_061152_reward_DEEPRA.png (deflated 9%)\nupdating: kaggle/working/output/20250620_055646_mmwave_rate_DEEPRA.png (deflated 5%)\nupdating: kaggle/working/output/20250620_064049_log_DEEPRA.txt (deflated 78%)\nupdating: kaggle/working/output/20250620_064049_plr_DEEPRA.png (deflated 7%)\nupdating: kaggle/working/output/20250620_064543_reward_DEEPRA.png (deflated 13%)\nupdating: kaggle/working/output/20250620_064049_mmwave_rate_DEEPRA.png (deflated 5%)\nupdating: kaggle/working/output/20250620_063554_log_DEEPRA.txt (deflated 78%)\nupdating: kaggle/working/output/20250620_062555_plr_DEEPRA.png (deflated 8%)\nupdating: kaggle/working/output/20250620_063554_sub6_rate_DEEPRA.png (deflated 3%)\nupdating: kaggle/working/output/20250620_062555_sub6_rate_DEEPRA.png (deflated 3%)\nupdating: kaggle/working/output/20250620_064543_choice_distribution_DEEPRA.png (deflated 23%)\nupdating: kaggle/working/output/20250620_064049_metric_DEEPRA.png (deflated 17%)\nupdating: kaggle/working/output/20250620_064049_sub6_rate_DEEPRA.png (deflated 3%)\nupdating: kaggle/working/output/20250620_064049_reward_DEEPRA.png (deflated 12%)\nupdating: kaggle/working/output/20250620_063554_mmwave_rate_DEEPRA.png (deflated 4%)\nupdating: kaggle/working/output/20250620_064543_plr_DEEPRA.png (deflated 7%)\nupdating: kaggle/working/output/20250620_064543_mmwave_rate_DEEPRA.png (deflated 5%)\nupdating: kaggle/working/output/20250620_064543_metric_DEEPRA.png (deflated 15%)\nupdating: kaggle/working/output/20250620_062555_metric_DEEPRA.png (deflated 16%)\nupdating: kaggle/working/output/20250620_063554_metric_DEEPRA.png (deflated 18%)\nupdating: kaggle/working/output/20250620_063058_log_DEEPRA.txt (deflated 77%)\nupdating: kaggle/working/output/20250620_063058_sub6_rate_DEEPRA.png (deflated 3%)\nupdating: kaggle/working/output/20250620_063058_mmwave_rate_DEEPRA.png (deflated 5%)\nupdating: kaggle/working/output/20250620_063554_plr_DEEPRA.png (deflated 7%)\nupdating: kaggle/working/output/20250620_062555_log_DEEPRA.txt (deflated 78%)\nupdating: kaggle/working/output/20250620_063058_choice_distribution_DEEPRA.png (deflated 23%)\nupdating: kaggle/working/output/20250620_062555_mmwave_rate_DEEPRA.png (deflated 5%)\nupdating: kaggle/working/output/20250620_064049_choice_distribution_DEEPRA.png (deflated 23%)\nupdating: kaggle/working/output/20250620_064543_sub6_rate_DEEPRA.png (deflated 3%)\nupdating: kaggle/working/output/20250620_063058_plr_DEEPRA.png (deflated 8%)\nupdating: kaggle/working/output/20250620_062555_reward_DEEPRA.png (deflated 15%)\nupdating: kaggle/working/output/20250620_063554_choice_distribution_DEEPRA.png (deflated 23%)\nupdating: kaggle/working/output/20250620_063058_metric_DEEPRA.png (deflated 17%)\nupdating: kaggle/working/output/20250620_063058_reward_DEEPRA.png (deflated 10%)\nupdating: kaggle/working/output/20250620_063554_reward_DEEPRA.png (deflated 13%)\nupdating: kaggle/working/output/20250620_064543_log_DEEPRA.txt (deflated 78%)\nupdating: kaggle/working/output/20250620_062555_choice_distribution_DEEPRA.png (deflated 23%)\n  adding: kaggle/working/output/20250620_065254_plr_DEEP.png (deflated 7%)\n  adding: kaggle/working/output/20250620_065430_sub6_rate_DEEP.png (deflated 3%)\n  adding: kaggle/working/output/20250620_065606_choice_distribution_DEEP.png (deflated 23%)\n  adding: kaggle/working/output/20250620_065915_metric_DEEP.png (deflated 15%)\n  adding: kaggle/working/output/20250620_065915_choice_distribution_DEEP.png (deflated 23%)\n  adding: kaggle/working/output/20250620_065254_mmwave_rate_DEEP.png (deflated 4%)\n  adding: kaggle/working/output/20250620_065430_plr_DEEP.png (deflated 8%)\n  adding: kaggle/working/output/20250620_065741_reward_DEEP.png (deflated 13%)\n  adding: kaggle/working/output/20250620_065606_mmwave_rate_DEEP.png (deflated 5%)\n  adding: kaggle/working/output/20250620_065741_choice_distribution_DEEP.png (deflated 23%)\n  adding: kaggle/working/output/20250620_065741_metric_DEEP.png (deflated 16%)\n  adding: kaggle/working/output/20250620_065741_log_DEEP.txt (deflated 78%)\n  adding: kaggle/working/output/20250620_065254_metric_DEEP.png (deflated 18%)\n  adding: kaggle/working/output/20250620_065254_log_DEEP.txt (deflated 78%)\n  adding: kaggle/working/output/20250620_065606_metric_DEEP.png (deflated 16%)\n  adding: kaggle/working/output/20250620_065430_reward_DEEP.png (deflated 13%)\n  adding: kaggle/working/output/20250620_065915_plr_DEEP.png (deflated 8%)\n  adding: kaggle/working/output/20250620_065430_metric_DEEP.png (deflated 18%)\n  adding: kaggle/working/output/20250620_065915_log_DEEP.txt (deflated 78%)\n  adding: kaggle/working/output/20250620_065606_sub6_rate_DEEP.png (deflated 3%)\n  adding: kaggle/working/output/20250620_065254_reward_DEEP.png (deflated 14%)\n  adding: kaggle/working/output/20250620_065915_sub6_rate_DEEP.png (deflated 3%)\n  adding: kaggle/working/output/20250620_065606_reward_DEEP.png (deflated 11%)\n  adding: kaggle/working/output/20250620_065430_choice_distribution_DEEP.png (deflated 23%)\n  adding: kaggle/working/output/20250620_065254_choice_distribution_DEEP.png (deflated 23%)\n  adding: kaggle/working/output/20250620_065741_mmwave_rate_DEEP.png (deflated 4%)\n  adding: kaggle/working/output/20250620_065606_plr_DEEP.png (deflated 8%)\n  adding: kaggle/working/output/20250620_065254_sub6_rate_DEEP.png (deflated 3%)\n  adding: kaggle/working/output/20250620_065430_log_DEEP.txt (deflated 78%)\n  adding: kaggle/working/output/20250620_065606_log_DEEP.txt (deflated 78%)\n  adding: kaggle/working/output/20250620_065741_plr_DEEP.png (deflated 7%)\n  adding: kaggle/working/output/20250620_065915_mmwave_rate_DEEP.png (deflated 4%)\n  adding: kaggle/working/output/20250620_065430_mmwave_rate_DEEP.png (deflated 4%)\n  adding: kaggle/working/output/20250620_065915_reward_DEEP.png (deflated 16%)\n  adding: kaggle/working/output/20250620_065741_sub6_rate_DEEP.png (deflated 3%)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# teho 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 79879\nprint(\"23:17\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:59:25.153049Z","iopub.execute_input":"2025-06-20T06:59:25.153604Z","iopub.status.idle":"2025-06-20T06:59:25.157437Z","shell.execute_reply.started":"2025-06-20T06:59:25.153576Z","shell.execute_reply":"2025-06-20T06:59:25.156980Z"}},"outputs":[{"name":"stdout","text":"23:17\n","output_type":"stream"}],"execution_count":20}]}